{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file builds a Mallet LDA model, and saves results/visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some terminologies:\n",
    "1. raw_doc: unprocessed raw document from txt file\n",
    "2. docs: lemmentized corpus\n",
    "3. corpus_bow: bag of words corpus\n",
    "4. corpus_tfidf: tfidf corpus\n",
    "\n",
    "#### Change from eariler version:\n",
    "1. filter out documents with too few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary and pre-built functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "#from collections import Counter\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global folder path \n",
    "data_folder = '../../data/'\n",
    "model_folder = '../../model/'\n",
    "raw_data_path = os.path.join(data_folder,'raw/article_IV_corpus.txt')\n",
    "data_processed_folder = os.path.join(data_folder,'processed')\n",
    "results_folder = os.path.join(data_folder,'results','temp_results')\n",
    "## binary file for mallet model\n",
    "mallet_path = '/mnt/packages/Mallet/bin/mallet' # update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load topic_models.py\n",
    "# python_root = './scripts'\n",
    "# sys.path.insert(0, python_root)\n",
    "\n",
    "#%%\n",
    "def prepare_data(data_folder,save=True):\n",
    "    ## read and transform data \n",
    "    contents = pickle.load(open(os.path.join(data_folder,'lemma_corpus.p'), \"rb\"))\n",
    "    print('length of lemmentized corpus: {}'.format(len(contents)))\n",
    "    docs = list()\n",
    "    for paragraph in contents:\n",
    "        docs.append([w for sentance in paragraph for w in sentance])\n",
    "\n",
    "    # build dictionary\n",
    "\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=5,no_above=0.5, keep_n=10000)\n",
    "    # convert document into bow\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in docs]\n",
    "    ## comput tfidf feature vectors\n",
    "    tfidf = models.TfidfModel(corpus_bow) # smartirs = 'atc' https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "    corpus_tfidf = tfidf[corpus_bow]\n",
    "    \n",
    "    ## save dictionary and corpora \n",
    "    if save:\n",
    "        dictionary_save_path = os.path.join(data_folder,'dictionary.dict')\n",
    "        dictionary.compactify()\n",
    "        dictionary.save(dictionary_save_path)\n",
    "        corpora.MmCorpus.serialize( os.path.join(data_folder,'corpus_bow.mm'), corpus_bow)\n",
    "        corpora.MmCorpus.serialize( os.path.join(data_folder,'corpus_tfidf.mm'), corpus_tfidf)\n",
    "        #print(len(dictionary))\n",
    "    return docs,dictionary,corpus_bow,corpus_tfidf\n",
    "\n",
    "#%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original text to look through later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(raw_data_path,'r',encoding='utf8') as f:\n",
    "    raw_doc = f.readlines()\n",
    "    raw_doc = [l.strip(' \\n') for l in raw_doc if len(l)>50]\n",
    "\n",
    "print('Length of raw documents {}'.format(len(raw_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load lemmentized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun = True\n",
    "if rerun == True:                ## save gensim objects, corpus, dictionary, and lda model\n",
    "    mode = 'all'\n",
    "    docs,dictionary,corpus_bow,corpus_tfidf = prepare_data(data_processed_folder,save=False)\n",
    "    # corpus_bow = [c for c in corpus_bow_full if len(c)>0]\n",
    "    \n",
    "print('Length of length of bag-of-word corpus: {}'.format(len(corpus_bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out paragraphs with <20 words or contain 'titles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc_new = list()\n",
    "corpus_bow_new = list()\n",
    "docs_new = list()\n",
    "\n",
    "tuple_temp = [(x, y, z) for (x, y, z) in zip(raw_doc, corpus_bow, docs) if len(x.split())>20 and ('<Title>' not in x) ]\n",
    "\n",
    "raw_doc_new, corpus_bow_new, docs_new = zip(*tuple_temp)\n",
    "\n",
    "print('Length of corpus without \"<Title>\" and has more than 20 words: {}'.format(len(raw_doc_new)))\n",
    "\n",
    "raw_doc_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA using Mallet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_words = 20\n",
    "np.random.seed(seed=1)\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus_bow_new, num_topics=n_topics, \n",
    "                                             id2word= dictionary, alpha= 1, optimize_interval=10, \n",
    "                                             iterations = 2000, \n",
    "                                             prefix=os.path.join(model_folder,\"mallet_{}_topics_\".format(n_topics)))\n",
    "#initial alpha = 5/ n_topics = 5/ 50 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('calculating coherence socre for {} documents ......'.format(len(docs_new)))\n",
    "coherence_model_lda = CoherenceModel(model=ldamallet, texts=docs_new, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_excel(model = ldamallet, n_topics = n_topics, n_words = n_words):\n",
    "    \n",
    "    '''saves results to excel for better visualization'''\n",
    "\n",
    "    # 1. Make a topic-key word table\n",
    "    topic_df = pd.DataFrame(data = np.zeros((n_topics, n_words)), columns= ['word'+ str(x) for x in range(n_words)])\n",
    "    for i in range(n_topics):\n",
    "        topic_df.iloc[i] = pd.DataFrame(model.show_topic(topicid= i, topn= n_words))[0].tolist()\n",
    "        \n",
    "    print(\"First step done\")\n",
    "    \n",
    "    # 2. Make a document-topic dataframe\n",
    "    docs_df = pd.DataFrame(data = np.zeros(len(docs_new)), columns=['paragraph'])\n",
    "    docs_df['paragraph'] = raw_doc_new\n",
    "    col_names = ['T'+ str(i) for i in np.array(range(n_topics))]\n",
    "\n",
    "    for col in col_names:\n",
    "        docs_df[col]= 0 \n",
    "\n",
    "    for row in range(docs_df.shape[0]):\n",
    "        tp = model[corpus_bow_new[row]]\n",
    "        for x in tp:\n",
    "            docs_df.loc[row, 'T'+ str(x[0])] = x[1]\n",
    "    \n",
    "    print(\"Second step done\")\n",
    "    \n",
    "    # 3. Make dataframe for topic-top document\n",
    "    \n",
    "    top_document_per_topic = []\n",
    "\n",
    "    for t_id in range(n_topics):\n",
    "        t = 'T'+ str(t_id)\n",
    "        print(\"Topic {}:\".format(t_id))\n",
    "        print(model.show_topic(topicid= t_id, topn=n_words))\n",
    "        print(str(raw_doc_new[docs_df[t].idxmax()]))\n",
    "        top_document_per_topic.append(str(raw_doc_new[docs_df[t].idxmax()]))\n",
    "\n",
    "    top_document_per_topic_df = pd.DataFrame(data = topic_df[topic_df.columns[0:]].apply(\n",
    "        lambda x: ','.join(x.astype(str)), axis =1), columns =['topic'])\n",
    "    \n",
    "    top_document_per_topic_df['top_document']= top_document_per_topic \n",
    "\n",
    "    # transform  document-toipc mapping from wide to long format\n",
    "    docs_df_long = pd.melt(docs_df,id_vars=['paragraph'], value_vars=['T'+ str(i) for i in range(50)] )\n",
    "    docs_df_long= docs_df_long[docs_df_long['value']>0]\n",
    "\n",
    "    docs_df_long.rename(columns={'variable':'topic','value':'probabiilty'}, inplace= True)\n",
    "    docs_df_long.topic = docs_df_long.topic.apply(lambda x: x.replace('T',''))\n",
    "          \n",
    "    # save results to excel\n",
    "    now = datetime.datetime.now()\n",
    "    now = now.strftime(\"%Y_%m_%d\")\n",
    "    #print(now)\n",
    "    writer = pd.ExcelWriter(path = os.path.join(results_folder,'Mallet_{}_topics_{}.xlsx'.format(n_topics, now)))\n",
    "        \n",
    "    top_document_per_topic_df.to_excel(writer,'Topic and Top Document')\n",
    "    topic_df.to_excel(writer, 'Toipc and Key Word')\n",
    "    docs_df_long.to_excel(writer, 'Document and Topic')\n",
    "    \n",
    "    writer.save()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some conversion from Mallet to Gensim LDA [for visualization](https://github.com/jerielizabeth/Gospel-of-Health-Notebooks/blob/master/blogPosts/pyLDAvis_and_Mallet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step was to extract the data from the MALLET statefile and into a pandas dataframe.\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "statefile = ldamallet.fstate() \n",
    "\n",
    "def extract_params(statefile):\n",
    "    \"\"\"Extract the alpha and beta values from the statefile.\n",
    "\n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        tuple: alpha (list), beta    \n",
    "    \"\"\"\n",
    "    with gzip.open(statefile, 'r') as state:\n",
    "        params = [x.decode('utf8').strip() for x in state.readlines()[1:3]]\n",
    "    return (list(params[0].split(\":\")[1].split(\" \")), float(params[1].split(\":\")[1]))\n",
    "\n",
    "\n",
    "def state_to_df(statefile):\n",
    "    \"\"\"Transform state file into pandas dataframe.\n",
    "    The MALLET statefile is tab-separated, and the first two rows contain the alpha and beta hypterparamters.\n",
    "    \n",
    "    Args:\n",
    "        statefile (str): Path to statefile produced by MALLET.\n",
    "    Returns:\n",
    "        datframe: topic assignment for each token in each document of the model\n",
    "    \"\"\"\n",
    "    return pd.read_csv(statefile,\n",
    "                       compression='gzip',\n",
    "                       sep=' ',\n",
    "                       skiprows=[1,2]\n",
    "                       )\n",
    "\n",
    "params = extract_params(statefile)\n",
    "\n",
    "alpha = [float(x) for x in params[0][1:]]\n",
    "beta = params[1]\n",
    "print(\"{}, {}\".format(alpha, beta))\n",
    "\n",
    "df = state_to_df(statefile)\n",
    "\n",
    "df['type'] = df.type.astype(str)\n",
    "df[:10]\n",
    "\n",
    "# Get document lengths from statefile\n",
    "docs = df.groupby('#doc')['type'].count().reset_index(name ='doc_length')\n",
    "\n",
    "docs[:10]\n",
    "\n",
    "# Get vocab and term frequencies from statefile\n",
    "vocab = df['type'].value_counts().reset_index()\n",
    "vocab.columns = ['type', 'term_freq']\n",
    "vocab = vocab.sort_values(by='type', ascending=True)\n",
    "\n",
    "vocab[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic-term matrix from state file\n",
    "# https://ldavis.cpsievert.me/reviews/reviews.html\n",
    "\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def pivot_and_smooth(df, smooth_value, rows_variable, cols_variable, values_variable):\n",
    "    \"\"\"\n",
    "    Turns the pandas dataframe into a data matrix.\n",
    "    Args:\n",
    "        df (dataframe): aggregated dataframe \n",
    "        smooth_value (float): value to add to the matrix to account for the priors\n",
    "        rows_variable (str): name of dataframe column to use as the rows in the matrix\n",
    "        cols_variable (str): name of dataframe column to use as the columns in the matrix\n",
    "        values_variable(str): name of the dataframe column to use as the values in the matrix\n",
    "    Returns:\n",
    "        dataframe: pandas matrix that has been normalized on the rows.\n",
    "    \"\"\"\n",
    "    matrix = df.pivot(index=rows_variable, columns=cols_variable, values=values_variable).fillna(value=0)\n",
    "    matrix = matrix.values + smooth_value\n",
    "    \n",
    "    normed = sklearn.preprocessing.normalize(matrix, norm='l1', axis=1)\n",
    "    \n",
    "    return pd.DataFrame(normed)\n",
    "\n",
    "phi_df = df.groupby(['topic', 'type'])['type'].count().reset_index(name ='token_count')\n",
    "phi_df = phi_df.sort_values(by='type', ascending=True)\n",
    "\n",
    "phi_df[:10]\n",
    "\n",
    "phi = pivot_and_smooth(phi_df, beta, 'topic', 'type', 'token_count')\n",
    "\n",
    "# phi[:10]\n",
    "\n",
    "theta_df = df.groupby(['#doc', 'topic'])['topic'].count().reset_index(name ='topic_count')\n",
    "\n",
    "theta_df[:10]\n",
    "\n",
    "theta = pivot_and_smooth(theta_df, alpha , '#doc', 'topic', 'topic_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create html visulaization using pyLDAvis\n",
    "# Note: Results are wrong because bugs in function malletmodel2ldamodel()\n",
    "\n",
    "data = {'topic_term_dists': phi, \n",
    "        'doc_topic_dists': theta,\n",
    "        'doc_lengths': list(docs['doc_length']),\n",
    "        'vocab': list(vocab['type']),\n",
    "        'term_frequency': list(vocab['term_freq'])\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "# ldamodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "viz_data= pyLDAvis.prepare(**data)\n",
    "viz_data\n",
    "# pyLDAvis.display(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y_%m_%d\")\n",
    "pyLDAvis.save_html(viz_data,  os.path.join(results_folder,'Mallet_{}_topics_{}.html'.format(n_topics, now))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a better way to print \n",
    "def print_topics_gensim(topic_model, total_topics=1,\n",
    "                        weight_threshold=0.0001,\n",
    "                        display_weights=False,\n",
    "                        num_terms=None):\n",
    "    \n",
    "    for index in range(total_topics):\n",
    "        topic = topic_model.show_topic(index,topn=num_terms)\n",
    "        topic = [(word, round(wt,4)) \n",
    "                 for word, wt in topic \n",
    "                 if abs(wt) >= weight_threshold]\n",
    "        if display_weights:\n",
    "            print('Topic #'+str(index+1)+' with weights')\n",
    "            print (topic[:num_terms] if num_terms else topic)\n",
    "        else:\n",
    "            print ('Topic #'+str(index+1)+' without weights')\n",
    "            tw = [term for term, wt in topic]\n",
    "            print (tw[:num_terms] if num_terms else tw)\n",
    "        print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topics_gensim(ldamallet,total_topics=n_topics,display_weights=False,num_terms=n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet.save(os.path.join(model_folder,'mallet_weights_{}_{}'.format(n_topics, now)))\n",
    "#test=gensim.models.wrappers.LdaMallet.load(os.path.join(model_folder,'mallet_weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this may take long time on domino \n",
    "# save_results_to_excel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
