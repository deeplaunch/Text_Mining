{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file adds result anaysis into a tuned LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some terminologies:\n",
    "1. raw_doc: unprocessed raw document from txt file\n",
    "2. docs: lemmentized corpus\n",
    "3. corpus_bow: bag of words corpus\n",
    "4. corpus_tfidf: tfidf corpus\n",
    "\n",
    "#### Change from eariler version:\n",
    "1. filter out documents with too few words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary and pre-built functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load topic_models.py\n",
    "\"\"\"\n",
    "Modified on Sun Dec 16 17:06:33 2018\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "#from collections import Counter\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "#import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "python_root = './scripts'\n",
    "sys.path.insert(0, python_root)\n",
    "\n",
    "#%%\n",
    "def prepare_data(save=True):\n",
    "    ## read and transform data \n",
    "    contents = pickle.load(open('../data/lemma_corpus.p', \"rb\"))\n",
    "    print('length of lemmentized corpus: {}'.format(len(contents)))\n",
    "    docs = list()\n",
    "    for paragraph in contents:\n",
    "        docs.append([w for sentance in paragraph for w in sentance])\n",
    "\n",
    "    # build dictionary\n",
    "\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=5,no_above=0.5, keep_n=10000)\n",
    "    # convert document into bow\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in docs]\n",
    "    ## comput tfidf feature vectors\n",
    "    tfidf = models.TfidfModel(corpus_bow) # smartirs = 'atc' https://radimrehurek.com/gensim/models/tfidfmodel.html\n",
    "    corpus_tfidf = tfidf[corpus_bow]\n",
    "    \n",
    "    ## save dictionary and corpora \n",
    "    if save:\n",
    "        dictionary_save_path = '../data/dictionary.dict'\n",
    "        dictionary.compactify()\n",
    "        dictionary.save(dictionary_save_path)\n",
    "        corpora.MmCorpus.serialize('../data/corpus_bow.mm', corpus_bow)\n",
    "        corpora.MmCorpus.serialize('../data/corpus_tfidf.mm', corpus_tfidf)\n",
    "        #print(len(dictionary))\n",
    "    return docs,dictionary,corpus_bow,corpus_tfidf\n",
    "\n",
    "#%%\n",
    "\n",
    "   \n",
    "def basic_lda(total_topics,corpus,dictionary,docs,score=False):\n",
    "    #total_topics = 15\n",
    "    print('Training for {} documents ......'.format(len(corpus)))\n",
    "    \n",
    "    lda = LdaModel(corpus = corpus,\n",
    "                              id2word = dictionary,\n",
    "                              num_topics = total_topics,\n",
    "                              alpha='auto',\n",
    "                              eta = 'auto',\n",
    "                              random_state = 2)#,\n",
    "                              #workers = 20) #\n",
    "                              #iterations = 1000,\n",
    "    # Compute Coherence Score\n",
    "    if score:\n",
    "        print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "        coherence_model_lda = CoherenceModel(model=lda, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "        return lda,coherence_lda\n",
    "    \n",
    "    return lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load original text to look through later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/article_IV_corpus.txt'\n",
    "\n",
    "with open(raw_data_path,'r',encoding='utf8') as f:\n",
    "    raw_doc = f.readlines()\n",
    "    raw_doc = [l.strip(' \\n') for l in raw_doc if len(l)>50]\n",
    "\n",
    "print('Length of raw documents {}'.format(len(raw_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load lemmentized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun = True\n",
    "if rerun == True:                ## save gensim objects, corpus, dictionary, and lda model\n",
    "    mode = 'all'\n",
    "    docs,dictionary,corpus_bow,corpus_tfidf = prepare_data(save=False)\n",
    "    # corpus_bow = [c for c in corpus_bow_full if len(c)>0]\n",
    "    \n",
    "print('Length of length of bag-of-word corpus: {}'.format(len(corpus_bow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out sample with <20 words or contain 'titles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc_new = list()\n",
    "corpus_bow_new = list()\n",
    "docs_new = list()\n",
    "\n",
    "tuple_temp = [(x, y, z) for (x, y, z) in zip(raw_doc, corpus_bow, docs) if len(x.split())>20 and ('<Title>' not in x) ]\n",
    "\n",
    "raw_doc_new, corpus_bow_new, docs_new = zip(*tuple_temp)\n",
    "\n",
    "print('Length of corpus without \"<Title>\" and has more than 20 words: {}'.format(len(raw_doc_new)))\n",
    "\n",
    "raw_doc_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 1 LDA model and get results for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 50 # number of topics assumed\n",
    "n_words = 20  # number of key words interested\n",
    "model, score = basic_lda(total_topics=n_topics,corpus=corpus_bow_new,dictionary=dictionary,docs=docs_new,score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: given document, get its topics\n",
    "doc_id = 0\n",
    "tp = model.get_document_topics(bow= corpus_bow_new[doc_id], minimum_probability= 0.17)\n",
    "print('tp: {}'.format(tp))\n",
    "for i in tp:\n",
    "    print('topic: {}'.format(i))\n",
    "    print(model.show_topic(topicid=i[0],topn=n_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic-key word table\n",
    "topic_df = pd.DataFrame(data = np.zeros((n_topics, n_words)), columns= ['word'+ str(x) for x in range(n_words)])\n",
    "for i in range(n_topics):\n",
    "    topic_df.iloc[i] = pd.DataFrame(model.show_topic(topicid= i, topn= n_words))[0].tolist()\n",
    "\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a document-topic datafram\n",
    "docs_df = pd.DataFrame(data = np.zeros(len(docs_new)), columns=['paragraph'])\n",
    "docs_df['paragraph'] = raw_doc_new\n",
    "\n",
    "col_names = ['T'+ str(i) for i in np.array(range(n_topics))]\n",
    "for col in col_names:\n",
    "    docs_df[col]= 0 \n",
    "\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(docs_df.shape[0]):\n",
    "    tp = model.get_document_topics(bow= corpus_bow_new[row], minimum_probability= 0.2)\n",
    "    for x in tp:\n",
    "        docs_df.loc[row, 'T'+ str(x[0])] = x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: given topic, find most possible document\n",
    "top_document_per_topic = []\n",
    "\n",
    "for t_id in range(n_topics):\n",
    "    t = 'T'+ str(t_id)\n",
    "    print(\"Topic {}:\".format(t_id))\n",
    "    print(model.show_topic(topicid= t_id, topn=n_words))\n",
    "    print(str(raw_doc_new[docs_df[t].idxmax()]))\n",
    "    top_document_per_topic.append(str(raw_doc_new[docs_df[t].idxmax()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_document_per_topic_df = pd.DataFrame(data = topic_df[topic_df.columns[0:]].apply(\n",
    "    lambda x: ','.join(x.astype(str)), axis =1),\n",
    "                                         columns =['topic'])\n",
    "top_document_per_topic_df['top_document']= top_document_per_topic \n",
    "top_document_per_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "now = datetime.datetime.now()\n",
    "now = now.strftime(\"%Y_%m_%d\")\n",
    "print(now)\n",
    "writer = pd.ExcelWriter(path = '../../../analysis/Analysis_{}_topics_{}.xlsx'.format(n_topics, now) )\n",
    "top_document_per_topic_df.to_excel(writer,'Topic and Top Document')\n",
    "\n",
    "# save document-toipc mapping to excel\n",
    "#docs_df.to_excel(writer,'Document and Topic')\n",
    "# save toipc-key word to excel\n",
    "topic_df.to_excel(writer, 'Toipc and Key Word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform from wide to long format\n",
    "docs_df_long = pd.melt(docs_df,id_vars=['paragraph'], value_vars=['T'+ str(i) for i in range(50)] )\n",
    "docs_df_long= docs_df_long[docs_df_long['value']>0]\n",
    "\n",
    "docs_df_long.rename(columns={'variable':'topic','value':'probabiilty'}, inplace= True)\n",
    "\n",
    "docs_df_long.to_excel(writer, 'Document and Topic')\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visulaization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "viz_data= pyLDAvis.gensim.prepare(model, corpus_bow_new, dictionary)\n",
    "#pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(viz_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_json(viz_data, '../../../analysis/Analysis_{}_topics_{}.js'.format(n_topics, now) )\n",
    "pyLDAvis.save_html(viz_data, '../../../analysis/Analysis_{}_topics_{}.html'.format(n_topics, now) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## a better way to print \n",
    "# def print_topics_gensim(topic_model, total_topics=1,\n",
    "#                         weight_threshold=0.0001,\n",
    "#                         display_weights=False,\n",
    "#                         num_terms=None):\n",
    "    \n",
    "#     for index in range(total_topics):\n",
    "#         topic = topic_model.show_topic(index,topn=num_terms)\n",
    "#         topic = [(word, round(wt,4)) \n",
    "#                  for word, wt in topic \n",
    "#                  if abs(wt) >= weight_threshold]\n",
    "#         if display_weights:\n",
    "#             print('Topic #'+str(index+1)+' with weights')\n",
    "#             print (topic[:num_terms] if num_terms else topic)\n",
    "#         else:\n",
    "#             print ('Topic #'+str(index+1)+' without weights')\n",
    "#             tw = [term for term, wt in topic]\n",
    "#             print (tw[:num_terms] if num_terms else tw)\n",
    "#         print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fine_tune_lda(corpus, dictionary, texts, limit, start=2, step=2):\n",
    "#     \"\"\"\n",
    "#     Compute c_v coherence for various number of topics\n",
    "#     Parameters:\n",
    "#     ----------\n",
    "#     dictionary : Gensim dictionary\n",
    "#     corpus : Gensim corpus\n",
    "#     texts : List of input texts\n",
    "#     limit : Max num of topics\n",
    "\n",
    "#     Returns:\n",
    "#     -------\n",
    "#     model_list : List of LDA topic models\n",
    "#     coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     n_topics : numbmber of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     n_topics = []\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         print('\\nTraing with n_topics = {}, training sample = {}.'.format(num_topics,len(corpus)))\n",
    "#         model = LdaModel(corpus = corpus,\n",
    "#                           id2word = dictionary,\n",
    "#                           random_state = 2,\n",
    "#                           alpha='auto',\n",
    "#                           eta = 'auto',\n",
    "#                           num_topics = num_topics)#\n",
    "#                           #distributed = True)  # alpha='auto' is not implenented in distributed lda\n",
    "#         model_list.append(model)\n",
    "#         print('Calculating coherence score based on {} samples.'.format(len(texts)))\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "#         n_topics.append(num_topics)\n",
    "#         print(\"{}: {}\".format(num_topics,coherence_values[-1]))\n",
    "        \n",
    "\n",
    "#     return model_list, coherence_values,n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%\n",
    "# if __name__== \"__main__\":\n",
    "    \n",
    "#     save = True  ## save gensim objects, corpus, dictionary, and lda model\n",
    "#     mode = 'all'\n",
    "#     docs,dictionary,corpus_bow,corpus_tfidf = prepare_data(save=save)\n",
    "#     corpus_bow = [c for c in corpus_bow if len(c)>0]\n",
    "    \n",
    "#     if mode == 'lda' or mode=='all':\n",
    "#         n_topics = 25\n",
    "#         model, score = basic_lda(total_topics=n_topics,corpus=corpus_bow,dictionary=dictionary,docs=docs,score=True)\n",
    "#         print(score)\n",
    "#         print_topics_gensim(topic_model=model,\n",
    "#                            total_topics = n_topics,\n",
    "#                            num_terms=20,\n",
    "#                            display_weights=True)\n",
    "#     if mode =='seed_lda' or mode=='all':\n",
    "#         n_topics = 25\n",
    "#         boost = 1000\n",
    "#         seed_topic_list = [['mpm','MPM','CFM','cfm','ltv','LTC','DSTI','dsti','lcr','LCR',\n",
    "#                             'capital_buffer','macroprudential','capital_flow','prudential'],\n",
    "#                             ['population','ageing','pension','productivity','migration','migrat']]\n",
    "            \n",
    "#         seed_model = seeded_lda(n_topics,corpus_bow,dictionary,docs,seed_topic_list, boost, score=False)\n",
    "#         ## for some reason keeps buging out when calculating coherence score \n",
    "        \n",
    "#         print_topics_gensim(topic_model=seed_model,\n",
    "#                            total_topics = n_topics,\n",
    "#                            num_terms=20,\n",
    "#                            display_weights=True)\n",
    "    \n",
    "#     if mode == 'fine_tune' or mode =='all':\n",
    "        \n",
    "#         model_list, coherence_values,n_topics = fine_tune_lda(dictionary=dictionary, corpus=corpus_bow,\n",
    "#                                                             texts=docs, start=15, limit=35, step=1)\n",
    "        \n",
    "#         best_model = model_list[np.argmax(coherence_values)]\n",
    "#         best_topic_n = best_model.get_topics().shape[0]\n",
    "        \n",
    "#         plt.plot(n_topics, coherence_values)\n",
    "#         plt.show()\n",
    "        \n",
    "#         print_topics_gensim(topic_model=best_model,\n",
    "#                        total_topics = best_topic_n,\n",
    "#                        num_terms=10,\n",
    "#                        display_weights=True)\n",
    "#         if save:\n",
    "#             lda_model_filepath = '../data/lda_res'\n",
    "#             best_model.save(lda_model_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mallet_lda(model_path,total_topics,corpus,dictionary,docs,score=False):\n",
    "#     \"\"\"\n",
    "#     https://radimrehurek.com/gensim/models/wrappers/ldamallet.html\n",
    "#     sudo apt-get install default-jdk\n",
    "#     sudo apt-get install ant\n",
    "#     git clone git@github.com:mimno/Mallet.git\n",
    "#     cd Mallet/\n",
    "#     ant\n",
    "    \n",
    "#     we don't have those packages in server environment\n",
    "#     \"\"\"\n",
    "#     lda = gensim.models.wrappers.LdaMallet(model_path, corpus=corpus, num_topics=total_topics, id2word=dictionary)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=lda, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return lda,coherence_score\n",
    "    \n",
    "# def hdp(corpus,dictionary,docs,score=False):\n",
    "#     print('Traiing for {} documents ......'.format(len(corpus)))\n",
    "#     hdpmodel = HdpModel(corpus = corpus,id2word = dictionary)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=hdpmodel, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return hdpmodel,coherence_score\n",
    "#     return hdpmodel\n",
    "    \n",
    "# def lsi(total_topics, corpus,dictionary,docs,score=False):\n",
    "#     print('Traiing for {} documents ......'.format(len(corpus)))\n",
    "#     lsimodel = LsiModel(corpus = corpus,id2word = dictionary,num_topics=total_topics)\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model = CoherenceModel(model=lsimodel, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_score = coherence_model.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_score)\n",
    "#         return lsimodel,coherence_score\n",
    "#     return lsimodel\n",
    "\n",
    "# def seeded_lda(total_topics,corpus,dictionary,docs,seed_topic_list, boost, score=False):\n",
    "#     print('Modify beta prior ......')\n",
    "#     _model = LdaModel(corpus = corpus_bow, id2word = dictionary,random_state = 2,alpha='auto',num_topics = total_topics,iterations=0)\n",
    "#     beta_matrix = _model.expElogbeta\n",
    "#     for t_id, st in enumerate(seed_topic_list):\n",
    "#         for word in st:\n",
    "#             try:\n",
    "#                 w_id = dictionary.token2id[word]\n",
    "#                 beta_matrix[t_id,w_id] = boost\n",
    "#                 print('{} : {} : {}'.format(t_id,w_id,word))\n",
    "#             except:\n",
    "#                 continue\n",
    "#     print('Training for {} documents ......'.format(len(corpus)))\n",
    "#     seed_model = LdaModel(corpus = corpus_bow,\n",
    "#                                   id2word = dictionary,\n",
    "#                                   num_topics = total_topics,\n",
    "#                                   eta = beta_matrix,\n",
    "#                                   random_state=2)\n",
    "#     # Compute Coherence Score\n",
    "#     if score:\n",
    "#         print('calculating coherence socre for {} documents ......'.format(len(docs)))\n",
    "#         coherence_model_lda = CoherenceModel(model=seed_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_lda = coherence_model_lda.get_coherence()\n",
    "#         print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#         return seed_model,coherence_lda\n",
    "    \n",
    "#     return seed_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
