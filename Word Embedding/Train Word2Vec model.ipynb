{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train word2vec from raw article IV reports using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminologies:\n",
    "1. raw_doc: unprocessed raw document from txt file\n",
    "\n",
    "#### Features:\n",
    "1. filter out certain punctuations\n",
    "2. replace numeric values with \"numeric_value\"\n",
    "3. choose embedding size of 100 due to small corpus size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dictionary and pre-built functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "import spacy\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global folder path \n",
    "data_folder = '../../data/'\n",
    "model_folder = '../../model/'\n",
    "raw_data_path = os.path.join(data_folder,'raw/article_IV_corpus.txt')\n",
    "data_processed_folder = os.path.join(data_folder,'processed')\n",
    "results_folder = os.path.join(data_folder,'results','topic_model_results')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "#nlp.pipeline = [nlp.tagger, nlp.sentencizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process original text using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw documents 142564\n"
     ]
    }
   ],
   "source": [
    "with open(raw_data_path,'r',encoding='utf8') as f:\n",
    "    raw_doc = f.readlines()\n",
    "    raw_doc = [l.strip(' \\n') for l in raw_doc if len(l)>50]\n",
    "\n",
    "print('Length of raw documents {}'.format(len(raw_doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2253.9872109889984 seconds --\n"
     ]
    }
   ],
   "source": [
    "def clean_sentence(sent):\n",
    "    '''remove punctuations in sentences\n",
    "       and reduce to lemma form'''\n",
    "    \n",
    "    sent = [word.lemma_ for word in sent if word.pos_ not in ('PUNCT')]\n",
    "    sent = [re.sub('[0-9]*.*[0-9]','numeric_value', word) for word in sent]\n",
    "    \n",
    "    return sent\n",
    "\n",
    "\n",
    "def prepare_data(raw_doc):\n",
    "    '''filter and lemmantize using spacy'''\n",
    "    lemma_doc = []\n",
    "    \n",
    "    for paragraph in raw_doc:        \n",
    "        doc = nlp(paragraph)\n",
    "        sents = list(doc.sents)\n",
    "        sentence = list(map(lambda x: clean_sentence(x), sents))\n",
    "        lemma_doc.append(sentence)\n",
    "    \n",
    "    ## flatten\n",
    "    lemma_doc = [s for l in lemma_doc for s in l ]\n",
    "\n",
    "    return lemma_doc\n",
    "\n",
    "start_time = time.time()\n",
    "processed_doc = prepare_data(raw_doc = raw_doc)\n",
    "print(\"--{} seconds --\".format(time.time()- start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(os.path.join(data_processed_folder,'processed_doc_for_word2vec_training.p'),\"wb\")\n",
    "pickle.dump(processed_doc, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train w2v model using gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize model and build vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 100\n",
    "window = 7 \n",
    "downsampling = 1e-5\n",
    "seed = 1\n",
    "num_workers = os.cpu_count()    ## not sure if this is a good idea\n",
    "min_count = 30 \n",
    "\n",
    "aiv_w2v = Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=n_dim,\n",
    "    min_count=min_count,\n",
    "    window= window,\n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "## build the vocabulary\n",
    "aiv_w2v.build_vocab(processed_doc)\n",
    "corpus_count = aiv_w2v.corpus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train w2v model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--574.172180891037 seconds --\n"
     ]
    }
   ],
   "source": [
    "iteration = 200\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if gensim.__version__[0] =='1':\n",
    "    aiv_w2v.train(processed_doc)\n",
    "else:\n",
    "    aiv_w2v.train(processed_doc,total_examples=corpus_count,epochs = iteration)\n",
    "    \n",
    "print(\"--{} seconds --\".format(time.time()- start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save trained word2 to vect model \n",
    "aiv_w2v.save(os.path.join(model_folder,'word2vec','aiv.w2v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6728"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fight', 0.7006235122680664),\n",
       " ('anti', 0.6929948329925537),\n",
       " ('anticorruption', 0.6784272789955139),\n",
       " ('governance', 0.6521619558334351),\n",
       " ('fraud', 0.6335301995277405),\n",
       " ('crime', 0.6289873123168945),\n",
       " ('judiciary', 0.6151541471481323),\n",
       " ('judicial', 0.6075106859207153),\n",
       " ('investigation', 0.6053670048713684),\n",
       " ('combat', 0.594965934753418),\n",
       " ('evasion', 0.555034339427948),\n",
       " ('perception', 0.5436075329780579),\n",
       " ('prosecution', 0.5430848002433777),\n",
       " ('corrupt', 0.5416703224182129),\n",
       " ('climate', 0.535169780254364),\n",
       " ('prosecute', 0.5312641263008118),\n",
       " ('laundering', 0.5267109870910645),\n",
       " ('procurement', 0.5227187275886536),\n",
       " ('enforcement', 0.5216412544250488),\n",
       " ('investigate', 0.5117053985595703)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = aiv_w2v.wv\n",
    "vocabs = model.vocab.keys()\n",
    "model.most_similar('corruption',topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
