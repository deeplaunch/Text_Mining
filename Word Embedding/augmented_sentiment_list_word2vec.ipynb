{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data: 1. lists with positive and negative words; 2. economic news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/apple/Desktop/Python/Text Mining/Word Embedding'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "word_dict= pd.read_csv(os.path.join('../Sentiment Analysis','pos_neg_list.csv'))\n",
    "pos_list = word_dict[word_dict['Positive']==1]['Word'].tolist()\n",
    "neg_list = word_dict[word_dict['Negative']==1]['Word'].tolist()\n",
    "\n",
    "# define negation word list\n",
    "negation_list = ['not','no','nobody','none','never','neither','cannot']\n",
    "\n",
    "# 2.\n",
    "full_df = pd.read_csv(os.path.join('../Sentiment Analysis',\"economic_sentiment_data.csv\") )\n",
    "full_df = full_df[['sentence','sentiment','polarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean and tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw):\n",
    "    '''clean a paragraph and breaks into sentences'''\n",
    "    raw = re.sub(r\"</br>\",\".\", raw)\n",
    "    raw = re.sub(r\"[.]+\",\".\", raw)\n",
    "    raw = re.sub(r\"[-+]?\\d*\\.\\d+|\\d+\",\"\", raw)\n",
    "    raw = re.sub(\"\\d\",\"\", raw)\n",
    "    raw = re.sub(r'[%-]',\"\", raw)\n",
    "    raw = sent_tokenize(raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = full_df.sentence.tolist()\n",
    "\n",
    "paragraphs = list(map(clean_text, paragraphs))\n",
    "\n",
    "sentences = [sent for para in paragraphs for sent in para ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    '''clean and tokenize each sentence into words'''\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "sentences = list(map(sentence_to_wordlist, sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply pre-trained word2vec to the positive and negative word lists to find most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. download word2vec model\n",
    "# ##specify download path and extract path \n",
    "# download_path = \"imf_w2v.zip\"\n",
    "# download_link = \"https://www.dropbox.com/sh/6um97x52kweebfx/AACSxB0E9weItCbyQwUqvuWRa?dl=1\"\n",
    "# extract_path = './data'\n",
    "# data_util.download_data(download_path,download_link,extract_path)\n",
    "\n",
    "#2. load pre-trained imf w2v model\n",
    "data_path = os.path.join('model','imf_160.w2v')\n",
    "imf_w2v = Word2Vec.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate new negative list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original negative list length: 295\n",
      "Thse in the word2vec vocab: 288\n"
     ]
    }
   ],
   "source": [
    "# filter out those not in word2vec vocab\n",
    "print('Original negative list length: {}'.format(len(neg_list)))\n",
    "neg_list_org = [x for x in neg_list if x in imf_w2v.wv.vocab] \n",
    "print('Thse in the word2vec vocab: {}'.format(len(neg_list_org)))\n",
    "neg_list_df = pd.DataFrame(neg_list_org, columns=['original_word']) # useful for merger later\n",
    "\n",
    "# get augmented list\n",
    "neg_df = list(map(lambda a: [[x[0],x[1]] for x in imf_w2v.wv.most_similar(a)], neg_list_org)) \n",
    "neg_df = list(map(pd.DataFrame, neg_df))\n",
    "neg_df = pd.concat(neg_df,axis = 0)\n",
    "\n",
    "neg_df['similarity_rank'] = neg_df.index\n",
    "neg_df.rename(columns={0: 'word', 1: 'similarity'},inplace= True)\n",
    "\n",
    "# merge with original word\n",
    "neg_df['original_word_rank'] = np.repeat(range(len(neg_list_org)),10)\n",
    "neg_df = neg_df.merge(neg_list_df, how = 'outer', left_on= 'original_word_rank', right_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new positive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_list))\n",
    "pos_list_int = [x for x in pos_list if x in imf_w2v.wv.vocab] # filter out those not in word2vec vocab\n",
    "print(len(pos_list_int))\n",
    "\n",
    "pos_list_df = pd.DataFrame(pos_list_int, columns=['original_word'])\n",
    "pos_list_df.head()\n",
    "\n",
    "pos_list_aug = list(map(lambda a: [[x[0],x[1]] for x in imf_w2v.wv.most_similar(a)], pos_list_int))\n",
    "\n",
    "pos_df = list(map(pd.DataFrame, pos_list_aug))\n",
    "\n",
    "pos_df = pd.concat(pos_df,axis = 0)\n",
    "\n",
    "pos_df['similarity_rank'] = pos_df.index\n",
    "\n",
    "pos_df.rename(columns={0: 'word', 1: 'similarity'},inplace= True)\n",
    "\n",
    "np.repeat(a=(1,2,3) , repeats= 10)\n",
    "\n",
    "pos_df['original_word_rank'] = np.repeat(range(len(pos_list_int)),10)\n",
    "\n",
    "pos_df = pos_df.merge(pos_list_df, how = 'outer', left_on= 'original_word_rank', right_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter and save based on: 1. similarity larger than 75 %ile; 2. new word not in the original word lists (pos & neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = pos_list + neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348, 6)\n",
      "(154, 6)\n"
     ]
    }
   ],
   "source": [
    "neg_df = neg_df[(neg_df.similarity >= 0.623453) & (~neg_df.word.isin(full_list))]\n",
    "neg_df['original_word_label'] = 'negative'\n",
    "print(neg_df.shape)\n",
    "\n",
    "#pos_df.describe()\n",
    "pos_df = pos_df[(pos_df.similarity >= 0.593445) & (~pos_df.word.isin(full_list))]\n",
    "pos_df['original_word_label'] = 'positive'\n",
    "print(pos_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([neg_df , pos_df], axis = 0)\n",
    "\n",
    "full_df = full_df.groupby('word',as_index= False).agg({'similarity': 'max',\n",
    "                                            'original_word_rank': 'count',\n",
    "                                            'original_word': 'first',\n",
    "                                            'similarity_rank':'mean',\n",
    "                                            'original_word_label':'first'})\n",
    "\n",
    "full_df.rename({'similarity': 'max_sim', 'original_word_rank': 'count_in_original_word','similarity_rank':'mean_rank',\n",
    "               }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path = 'aug_pos_neg_list.xlsx')\n",
    "        \n",
    "full_df.to_excel(writer, 'full_df')\n",
    "\n",
    "writer.save()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate sentiment score on new list (need to be manually labeled from the previous results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(paragraph, pos_list, neg_list, negation_list):\n",
    "    \n",
    "    '''return sentiment score, only negate positive words'''\n",
    "    \n",
    "    new_tokens = word_tokenize(paragraph)\n",
    "    new_tokens =[x.lower() for x in new_tokens]\n",
    "    \n",
    "    window = 3\n",
    "    \n",
    "    n_pos = sum([new_tokens.count(x) for x in pos_list])\n",
    "    n_neg = sum([new_tokens.count(x) for x in neg_list])\n",
    "    n_total = len(new_tokens)\n",
    "    \n",
    "    ## calculate number of negation words in the window of +/-3 next to n_pos \n",
    "    \n",
    "    pos_index = [i for i, val in enumerate(new_tokens) if val in pos_list]\n",
    "    pos_range_lower = np.array(pos_index) - window\n",
    "    pos_range_upper = np.array(pos_index) + window\n",
    "    \n",
    "    negation_index = [i for i, val in enumerate(new_tokens) if val in negation_list]\n",
    "    \n",
    "    \n",
    "    pos_range_lower = np.repeat(pos_range_lower, len(negation_index))    \n",
    "    pos_range_upper = np.repeat(pos_range_upper, len(negation_index))\n",
    "    \n",
    "    negation_index = np.repeat(negation_index, n_pos)\n",
    "    \n",
    "    n_negation = np.sum( (pos_range_lower < negation_index) & (pos_range_upper > negation_index) )\n",
    "    \n",
    "    sentiment_score = (n_pos-n_negation - n_neg) / n_total\n",
    "    \n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction and predict_lable using the above bow approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prediciton'] = full_df.sentence.apply(lambda x: get_sentiment_score(paragraph=x, pos_list= pos_list, neg_list= neg_list, negation_list= negation_list))\n",
    "\n",
    "full_df['predict_label'] = full_df.prediciton.apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for full sample and sub samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(full_df['polarity'], full_df['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(full_df.iloc[:3000]['polarity'], full_df.iloc[:3000]['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(full_df.iloc[3000:]['polarity'], full_df.iloc[3000:]['predict_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  '''While the RMB in 2017 was broadly in line with\n",
    "economic fundamentals and desirable policies, the current account surplus was moderately\n",
    "stronger. This reflects structural distortions and policies that cause excessive savings, such as low\n",
    "social spending. Addressing these distortions and the resulting external imbalance would benefit\n",
    "both China and the global economy.''',\n",
    "  '''Favorable domestic and external conditions reduced capital outflows and exchange\n",
    "rate pressure. The RMB was broadly stable against the basket published by the China Foreign\n",
    "Exchange Trade System (CFETS) in 2017, but with more fluctuation versus the dollar, and it has\n",
    "appreciated by about 2 percent in real effective terms in the first half of 2018. The current account\n",
    "surplus continued to decline but, reflecting distortions and policy gaps that encourage excessive\n",
    "savings, the external position for 2017 is assessed as moderately stronger than the level consistent\n",
    "with medium-term fundamentals and desirable policies, with the exchange rate broadly in line\n",
    "(Appendix I).''',\n",
    "    '''Large outflows and pressure on\n",
    "the exchange rate could resume due to tighter\n",
    "and more volatile global financial conditions,\n",
    "especially a surging dollar. Investor sentiment\n",
    "towards emerging markets has recently\n",
    "weakened, and this could intensify, potentially\n",
    "spreading to China.''',\n",
    "  '''. Uncoordinated financial and local government regulatory action could have\n",
    "unintended consequences that trigger disorderly repricing of corporate/LGFV credit risks, losses\n",
    "for investors, and rollover risks for financial institutions''',\n",
    "  '''But a lack of decisive reforms in deleveraging and rebalancing would add to the\n",
    "Faster reform progress could pave the way for higher and\n",
    "more sustainable GDP growth, already-high stock of vulnerabilities and worsen resource allocation, leading to more rapidly\n",
    "diminishing returns over the medium term. This scenario also raises the probability of a disruptive\n",
    "adjustment to Chinese demand which would result in a contractionary impulse to the global\n",
    "economy, as well as spillovers through commodity prices and financial markets. '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_sentiment_score(x, pos_list= pos_list, neg_list= neg_list, negation_list= negation_list) for x in pred_sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
