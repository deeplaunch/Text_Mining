{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn import metrics\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict= pd.read_csv('pos_neg_list.csv')\n",
    "pos_list = word_dict[word_dict['Positive']==1]['Word'].tolist()\n",
    "neg_list = word_dict[word_dict['Negative']==1]['Word'].tolist()\n",
    "\n",
    "# define negation word list\n",
    "negation_list = ['not','no','nobody','none','never','neither','cannot']\n",
    "\n",
    "full_df = pd.read_csv( \"economic_sentiment_data.csv\")\n",
    "\n",
    "full_df = full_df[['sentence','sentiment','polarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply word2vec to the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw):\n",
    "    raw = re.sub(r\"</br>\",\".\", raw)\n",
    "    raw = re.sub(r\"[.]+\",\".\", raw)\n",
    "    raw = re.sub(r\"[-+]?\\d*\\.\\d+|\\d+\",\"\", raw)\n",
    "    raw = re.sub(\"\\d\",\"\", raw)\n",
    "    raw = re.sub(r'[%-]',\"\", raw)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = full_df.sentence.tolist()\n",
    "\n",
    "paragraphs = list(map(clean_text, paragraphs))\n",
    "\n",
    "paragraphs = list(map(sent_tokenize,paragraphs))\n",
    "\n",
    "sentences = [sent for para in paragraphs for sent in para ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "sentences = list(map(sentence_to_wordlist, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained imf w2v model\n",
    "data_path = os.path.join('U:\\My Documents\\Python\\Text Mining\\Data\\w2v','imf_160.w2v')\n",
    "imf_w2v = Word2Vec.load(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gennerate new negative table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295\n",
      "288\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_list))\n",
    "neg_list_int = [x for x in neg_list if x in imf_w2v.wv.vocab] # filter out those not in word2vec vocab\n",
    "print(len(neg_list_int))\n",
    "\n",
    "neg_list_table = pd.DataFrame(neg_list_int, columns=['original_word'])\n",
    "neg_list_table.head()\n",
    "\n",
    "neg_list_aug = list(map(lambda a: [[x[0],x[1]] for x in imf_w2v.wv.most_similar(a)], neg_list_int))\n",
    "\n",
    "neg_table = list(map(pd.DataFrame, neg_list_aug))\n",
    "\n",
    "neg_table = pd.concat(neg_table,axis = 0)\n",
    "\n",
    "neg_table['distance_rank'] = neg_table.index\n",
    "\n",
    "neg_table.rename(columns={0: 'word', 1: 'distance'},inplace= True)\n",
    "\n",
    "np.repeat(a=(1,2,3) , repeats= 10)\n",
    "\n",
    "neg_table['original_word_rank'] = np.repeat(range(len(neg_list_int)),10)\n",
    "\n",
    "neg_table = neg_table.merge(neg_list_table, how = 'outer', left_on= 'original_word_rank', right_index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate new positive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "95\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_list))\n",
    "pos_list_int = [x for x in pos_list if x in imf_w2v.wv.vocab] # filter out those not in word2vec vocab\n",
    "print(len(pos_list_int))\n",
    "\n",
    "pos_list_table = pd.DataFrame(pos_list_int, columns=['original_word'])\n",
    "pos_list_table.head()\n",
    "\n",
    "pos_list_aug = list(map(lambda a: [[x[0],x[1]] for x in imf_w2v.wv.most_similar(a)], pos_list_int))\n",
    "\n",
    "pos_table = list(map(pd.DataFrame, pos_list_aug))\n",
    "\n",
    "pos_table = pd.concat(pos_table,axis = 0)\n",
    "\n",
    "pos_table['distance_rank'] = pos_table.index\n",
    "\n",
    "pos_table.rename(columns={0: 'word', 1: 'distance'},inplace= True)\n",
    "\n",
    "np.repeat(a=(1,2,3) , repeats= 10)\n",
    "\n",
    "pos_table['original_word_rank'] = np.repeat(range(len(pos_list_int)),10)\n",
    "\n",
    "pos_table = pos_table.merge(pos_list_table, how = 'outer', left_on= 'original_word_rank', right_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(path = 'aug_pos_neg_list.xlsx')\n",
    "        \n",
    "pos_table.to_excel(writer, 'pos')\n",
    "neg_table.to_excel(writer, 'neg')\n",
    "\n",
    "writer.save()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(paragraph, pos_list, neg_list, negation_list):\n",
    "    \n",
    "    '''return sentiment score, only negate positive words'''\n",
    "    \n",
    "    new_tokens = word_tokenize(paragraph)\n",
    "    new_tokens =[x.lower() for x in new_tokens]\n",
    "    \n",
    "    window = 3\n",
    "    \n",
    "    n_pos = sum([new_tokens.count(x) for x in pos_list])\n",
    "    n_neg = sum([new_tokens.count(x) for x in neg_list])\n",
    "    n_total = len(new_tokens)\n",
    "    \n",
    "    ## calculate number of negation words in the window of +/-3 next to n_pos \n",
    "    \n",
    "    pos_index = [i for i, val in enumerate(new_tokens) if val in pos_list]\n",
    "    pos_range_lower = np.array(pos_index) - window\n",
    "    pos_range_upper = np.array(pos_index) + window\n",
    "    \n",
    "    negation_index = [i for i, val in enumerate(new_tokens) if val in negation_list]\n",
    "    \n",
    "    \n",
    "    pos_range_lower = np.repeat(pos_range_lower, len(negation_index))    \n",
    "    pos_range_upper = np.repeat(pos_range_upper, len(negation_index))\n",
    "    \n",
    "    negation_index = np.repeat(negation_index, n_pos)\n",
    "    \n",
    "    n_negation = np.sum( (pos_range_lower < negation_index) & (pos_range_upper > negation_index) )\n",
    "    \n",
    "    sentiment_score = (n_pos-n_negation - n_neg) / n_total\n",
    "    \n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prediction and predict_lable using the above bow approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['prediciton'] = full_df.sentence.apply(lambda x: get_sentiment_score(paragraph=x, pos_list= pos_list, neg_list= neg_list, negation_list= negation_list))\n",
    "\n",
    "full_df['predict_label'] = full_df.prediciton.apply(lambda x: 1 if x>0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get accuracy for full sample and sub samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6605333333333333"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(full_df['polarity'], full_df['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(full_df.iloc[:3000]['polarity'], full_df.iloc[:3000]['predict_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6506666666666666"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(full_df.iloc[3000:]['polarity'], full_df.iloc[3000:]['predict_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Several Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  '''While the RMB in 2017 was broadly in line with\n",
    "economic fundamentals and desirable policies, the current account surplus was moderately\n",
    "stronger. This reflects structural distortions and policies that cause excessive savings, such as low\n",
    "social spending. Addressing these distortions and the resulting external imbalance would benefit\n",
    "both China and the global economy.''',\n",
    "  '''Favorable domestic and external conditions reduced capital outflows and exchange\n",
    "rate pressure. The RMB was broadly stable against the basket published by the China Foreign\n",
    "Exchange Trade System (CFETS) in 2017, but with more fluctuation versus the dollar, and it has\n",
    "appreciated by about 2 percent in real effective terms in the first half of 2018. The current account\n",
    "surplus continued to decline but, reflecting distortions and policy gaps that encourage excessive\n",
    "savings, the external position for 2017 is assessed as moderately stronger than the level consistent\n",
    "with medium-term fundamentals and desirable policies, with the exchange rate broadly in line\n",
    "(Appendix I).''',\n",
    "    '''Large outflows and pressure on\n",
    "the exchange rate could resume due to tighter\n",
    "and more volatile global financial conditions,\n",
    "especially a surging dollar. Investor sentiment\n",
    "towards emerging markets has recently\n",
    "weakened, and this could intensify, potentially\n",
    "spreading to China.''',\n",
    "  '''. Uncoordinated financial and local government regulatory action could have\n",
    "unintended consequences that trigger disorderly repricing of corporate/LGFV credit risks, losses\n",
    "for investors, and rollover risks for financial institutions''',\n",
    "  '''But a lack of decisive reforms in deleveraging and rebalancing would add to the\n",
    "Faster reform progress could pave the way for higher and\n",
    "more sustainable GDP growth, already-high stock of vulnerabilities and worsen resource allocation, leading to more rapidly\n",
    "diminishing returns over the medium term. This scenario also raises the probability of a disruptive\n",
    "adjustment to Chinese demand which would result in a contractionary impulse to the global\n",
    "economy, as well as spillovers through commodity prices and financial markets. '''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.05172413793103448,\n",
       " 0.008771929824561403,\n",
       " -0.045454545454545456,\n",
       " -0.06451612903225806,\n",
       " -0.047058823529411764]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[get_sentiment_score(x, pos_list= pos_list, neg_list= neg_list, negation_list= negation_list) for x in pred_sentences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
