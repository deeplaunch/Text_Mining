{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### inspired by https://github.com/explosion/spacy-pytorch-transformers/blob/master/examples/train_textcat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  load csv and convert to json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.getcwd()\n",
    "\n",
    "def load_directory_data(directory):\n",
    "  \n",
    "  # data = load_directory_data(os.path.join(directory, \"economic_sentiment_data.csv\"))\n",
    "\n",
    "  data = pd.read_csv(os.path.join(directory, \"economic_sentiment_data.csv\"))\n",
    "  \n",
    "  data = data[['sentence','polarity']]\n",
    "  \n",
    "  print(data.shape)\n",
    "\n",
    "  return data\n",
    "\n",
    "data_folder = '../data/'\n",
    "full_data_df = load_directory_data(data_folder)\n",
    "\n",
    "train_df = full_data_df.iloc[:3000]\n",
    "test_df = full_data_df.iloc[3000:]\n",
    "\n",
    "# convert to json\n",
    "train_df.to_json( os.path.join(data_folder, 'train.json'), orient= 'index')\n",
    "test_df.to_json(os.path.join(data_folder,'test.json'), orient= 'index', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load json as dictionary, convert to list of tuples as required by spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "train_json_str = train_df.to_json(orient= 'index')\n",
    "test_json_str = test_df.to_json(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_json = json.loads(train_json_str)\n",
    "test_json = json.loads(test_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA =[]\n",
    "\n",
    "for v in train_json.values():\n",
    "    TRAIN_DATA.append((v['sentence'],{'cats':{'POSITIVE': v['polarity'],'NEGATIVE': 1-v['polarity']}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_DATA =[]\n",
    "for v in test_json.values():\n",
    "    TEST_DATA.append((v['sentence'],{'cats':{'POSITIVE': v['polarity'],'NEGATIVE': 1-v['polarity']}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import rhinoscriptsyntax as rs\n",
    "# import json\n",
    "\n",
    "# #prompt the user for a file to import\n",
    "# filter = \"JSON file (*.json)|*.json|All Files (*.*)|*.*||\"\n",
    "# filename = rs.OpenFileName(\"Open JSON File\", filter)\n",
    "\n",
    "# with open(os.path.join(data_folder, 'train.json'),'r') as file:\n",
    "#     train_json = file.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch\n",
    "import random\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "nlp = spacy.load(\"en_pytt_bertbaseuncased_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentencizer', 'pytt_wordpiecer', 'pytt_tok2vec']\n",
      "['sentencizer', 'pytt_wordpiecer', 'pytt_tok2vec', 'pytt_textcat']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names) # [\"sentencizer\", \"pytt_wordpiecer\", \"pytt_tok2vec\"]\n",
    "textcat = nlp.create_pipe(\"pytt_textcat\", config={\"exclusive_classes\": True})\n",
    "for label in (\"POSITIVE\", \"NEGATIVE\"):\n",
    "    textcat.add_label(label)\n",
    "nlp.add_pipe(textcat)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training is very slow, 25 minutes per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'pytt_textcat': 0.011014029983925866}\n",
      "1 {'pytt_textcat': 0.008851727914588992}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-792aaa6092f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#scores = nlp.evaluate(TEST_DATA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#scores,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/spacy_pytorch_transformers/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             )\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mbackprop_tok2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresume_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/spacy_pytorch_transformers/tok2vec.py\u001b[0m in \u001b[0;36mfinish_update\u001b[0;34m(docs, sgd)\u001b[0m\n\u001b[1;32m    130\u001b[0m                     )\n\u001b[1;32m    131\u001b[0m                 )\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytt_d_last_hidden_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/spacy_pytorch_transformers/tok2vec.py\u001b[0m in \u001b[0;36msentence_bwd\u001b[0;34m(d_doc_acts, sgd)\u001b[0m\n\u001b[1;32m    382\u001b[0m             ]\n\u001b[1;32m    383\u001b[0m             \u001b[0md_sent_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_nested\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0md_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbp_sent_acts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_sent_acts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected gradient of sentence to be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mcontinue_update\u001b[0;34m(gradient, sgd)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/spacy_pytorch_transformers/tok2vec.py\u001b[0m in \u001b[0;36mbackprop_batched\u001b[0;34m(d_outputs, sgd)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0md_activs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_batch_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_activs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/spacy_pytorch_transformers/wrapper.py\u001b[0m in \u001b[0;36mbackward_pytorch\u001b[0;34m(d_output, sgd)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Gradients on all attentions not supported yet.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mFINE_TUNE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_for_bwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdy_for_bwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = nlp.resume_training()\n",
    "\n",
    "for i in range(3):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    for batch in minibatch(TRAIN_DATA, size=32):\n",
    "        texts, cats = zip(*batch)\n",
    "        nlp.update(texts, cats, sgd=optimizer, losses=losses, drop=0.1)\n",
    "    #scores = nlp.evaluate(TEST_DATA)\n",
    "    print(i, losses) #scores, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0 {'pytt_textcat': 0.0007305766448553186}\n",
    "##### 1 {'pytt_textcat': 0.0005921392457821639}\n",
    "##### 0 {'pytt_textcat': 0.0004447193523446913}\n",
    "##### 1 {'pytt_textcat': 0.0003327218119011377}\n",
    "##### 2 {'pytt_textcat': 0.000254198645507131}\n",
    "##### 0 {'pytt_textcat': 0.011014029983925866}\n",
    "##### 1 {'pytt_textcat': 0.008851727914588992}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(nlp, texts, cats):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm.tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=128)):\n",
    "            gold = cats[i]['cats']\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label == \"NEGATIVE\":\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_texts, eval_cats = zip(*TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'textcat_f': 0.6401273885146456,\n",
       " 'textcat_p': 0.5661971830826423,\n",
       " 'textcat_r': 0.7362637362367669}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(nlp, eval_texts[:], eval_cats[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###{'textcat_f': 0.6401273885146456,\n",
    "### 'textcat_p': 0.5661971830826423,\n",
    "### 'textcat_r': 0.7362637362367669}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some tests using articles from today's FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian shares endured their worst July in 17 years, a sign that stewing trouble in the country’s economy has been catching up with the stock market after a stellar run. {'POSITIVE': 0.0, 'NEGATIVE': 1.0}\n",
      "KKR has won the race to buy German payments group Heidelpay for more than €600m in a fresh sign of investor appetite for companies that offer digital alternatives to cash. {'POSITIVE': 1.0, 'NEGATIVE': 0.0}\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Indian shares endured their worst July in 17 years, a sign that stewing trouble in the country’s economy has been catching up with the stock market after a stellar run.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)\n",
    "\n",
    "test_text = \"KKR has won the race to buy German payments group Heidelpay for more than €600m in a fresh sign of investor appetite for companies that offer digital alternatives to cash.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../model/bert-textcat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
