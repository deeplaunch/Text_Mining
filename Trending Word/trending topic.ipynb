{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending topics\n",
    "\n",
    "### While topic modeling such as LDA seems to be an easy tool to identify topics, my past experience using it showed three drawbacks when it comes to our task of analyzing OCBC on Twitter.\n",
    "\n",
    "1. Once an unsupervised model is selected, classfications become static, which means \"unseen\" topics in the future won't be able to be captured.\n",
    "\n",
    "2. Twitter messages are very short and more likely to contain a homogenous topic than not, which challenges LDA's assumption of a probability distribution of \"multiple\" topics in each document.\n",
    "\n",
    "3. Ultimately LDA is a statistical model that isn't quite able capture similar semantics of words\n",
    "\n",
    "### Instead, here I build a *\"live\"* pipeline of trending topic detector combining several techniques such as Doc2Vec and hierachical clustering. This approach allws us to focus on recent documents, regardless what the past topic distribution might look like.\n",
    "\n",
    "\n",
    "### _Please note: here I don't have any API for news feed, thus using a file sent from a friend of mine. However the approach is similar._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import xlrd\n",
    "import matplotlib as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load and Clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8213, 19)\n",
      "Index(['DATE', 'TIME', 'UNIQUE_STORY_INDEX', 'EVENT_TYPE', 'PNAC',\n",
      "       'STORY_DATE_TIME', 'TAKE_DATE_TIME', 'HEADLINE_ALERT_TEXT',\n",
      "       'ACCUMULATED_STORY_TEXT', 'TAKE_TEXT', 'PRODUCTS', 'TOPICS',\n",
      "       'RELATED_RICS', 'NAMED_ITEMS', 'HEADLINE_SUBTYPE', 'STORY_TYPE',\n",
      "       'TABULAR_FLAG', 'ATTRIBUTION', 'LANGUAGE'],\n",
      "      dtype='object')\n",
      "(3137, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3137"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv('sample_file.csv') \n",
    "print(text_df.shape)\n",
    "print(text_df.columns)\n",
    "\n",
    "# remove rows that indicates \"please ignore\"\n",
    "text_df = text_df[text_df.HEADLINE_ALERT_TEXT.str.contains(\"Test, Please Ignore\")==False]\n",
    "text_df.LANGUAGE.value_counts()\n",
    "\n",
    "#### Only English\n",
    "text_df = text_df[text_df['LANGUAGE']=='EN']\n",
    "\n",
    "## Use Headline when no Take Text is available\n",
    "text_df.loc[text_df['TAKE_TEXT'].isnull(),'TAKE_TEXT'] = text_df.loc[text_df['TAKE_TEXT'].isnull(),'HEADLINE_ALERT_TEXT']\n",
    "\n",
    "## fill missing time\n",
    "text_df.DATE = text_df.DATE.fillna(method = 'ffill')\n",
    "text_df.TIME = text_df.TIME.fillna(method = 'ffill')\n",
    "\n",
    "## format time\n",
    "\n",
    "text_df.DATE = pd.to_datetime(text_df.DATE)\n",
    "text_df['HOUR'] = text_df.TIME.apply(lambda x: x.split(':')[0])\n",
    "\n",
    "# cleaning text\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.str.lower().str.strip().str.replace('[^\\w\\s]''',' ').str.replace('[^a-zA-Z0-9'' ]',' ').str.replace(r'\\W*\\b\\w{1,1}\\b', '')\n",
    "\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.apply(lambda x: re.sub(r'[\\*|\\+|\\_|\\-|\\<||>|\\(|\\)]','',x))\n",
    "\n",
    "text_df = text_df[text_df.TAKE_TEXT.notnull()]\n",
    "\n",
    "# apply gensim processing\n",
    "text_df['TAKE_TEXT'] = text_df['TAKE_TEXT'].apply(gensim.utils.simple_preprocess)\n",
    "print(text_df.shape)\n",
    "\n",
    "train_corpus = text_df.TAKE_TEXT.tolist()\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(value, [key])for key , value in enumerate(train_corpus)]\n",
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a Doc2Vec Model quickly (can also use a pre-trained model from wiki for transfer-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "# train model\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test of the model ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (732): «nymex palladium warehouse statistics pa stock»\n",
      "\n",
      "Similar Document (738, 0.8323554992675781): «comex silver warehouse statistics si stock»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random check to see similar documents are indeed identified (cltr-end for a few examples)\n",
    "doc_id = np.random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cluster documents for topic identification using Hierachical Agglomerative Clustering\n",
    "#### Here clustering is executed in each time window (4 hours as I choose). We are able to identify the top topic in each time window (my defiintion of \"trending\"). Such \"onlineness\" is a key feature of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def find_largest_topic(vector):\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters= 100, linkage='ward')  # Here we want many clusters to get smaller ones\n",
    "    cluster.fit_predict(vector)\n",
    "    max_id = most_common(list(cluster.labels_))\n",
    "    result_id = (cluster.labels_ == max_id)\n",
    "    \n",
    "    return result_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['vector'] = [model.infer_vector(x.words) for x in train_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break data into 4-hour window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = text_df\n",
    "result_df.HOUR = result_df.HOUR.apply(float)\n",
    "n = int(result_df.HOUR.max() / 4)\n",
    "result_df['Hour_4'] = pd.qcut(result_df.HOUR, n , labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list =[]\n",
    "\n",
    "for x in range(n):\n",
    "    vector = result_df.loc[result_df.Hour_4 == x, 'vector'].tolist()\n",
    "    top_topic_id = find_largest_topic(vector)\n",
    "    top_docs = result_df.loc[result_df.Hour_4 == x, 'TAKE_TEXT'][top_topic_id]\n",
    "    topic_list.append(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [mercator, minerals, announces, senior, manage...\n",
       "5       [mercator, minerals, announces, senior, manage...\n",
       "6       [korea, says, may, exports, pct, vs, yr, earli...\n",
       "11      [korea, says, may, imports, pct, vs, yr, earli...\n",
       "14               [star, buffet, inc, sells, real, estate]\n",
       "25                       [top, news, investment, banking]\n",
       "144     [tennis, vulnerable, nadal, needs, to, find, f...\n",
       "151     [service, alert, msci, world, and, us, eod, de...\n",
       "154     [service, alert, reuters, xtra, confirmed, see...\n",
       "419     [update, rebalance, to, asia, pacific, gaining...\n",
       "468     [peru, inflation, index, pecpi, eci, rises, pc...\n",
       "488     [corrected, mitsubishi, to, buy, grain, compan...\n",
       "561                        [india, call, money, pct, jun]\n",
       "609     [exxon, mobil, xom, reports, unplanned, flarin...\n",
       "610     [exxon, reports, unplanned, flaring, breakdown...\n",
       "613                           [update, baseball, results]\n",
       "1108                             [top, news, front, page]\n",
       "1110    [rugby, cooper, redeems, himself, to, help, re...\n",
       "1490    [update, stocks, news, mideast, saudi, mkt, cl...\n",
       "1942                                    [emea, test, rcf]\n",
       "1943    [by, ramkumar, koottala, hari, abcdefghijklmn,...\n",
       "2100    [soccer, barcelona, reach, points, with, rout,...\n",
       "2221    [update, leading, sunni, muslim, cleric, calls...\n",
       "2224                             [top, news, front, page]\n",
       "2227                             [top, news, front, page]\n",
       "2467                             [top, news, front, page]\n",
       "3105    [preview, soccer, all, to, play, for, in, clos...\n",
       "3107    [rpt, mideast, debt, issuers, warm, to, cross,...\n",
       "3202    [insight, nordic, nations, grapple, with, aust...\n",
       "3331    [motorcycling, motorcycling, grand, prix, moto...\n",
       "                              ...                        \n",
       "5272    [brief, national, australia, bank, plans, mln,...\n",
       "5602                                     [diary, vietnam]\n",
       "5607      [korea, may, manufacturing, pmi, vs, in, april]\n",
       "5616               [buzz, eur, aud, uptrend, strong, for]\n",
       "5622    [japan, corporate, capital, spending, falls, p...\n",
       "5634    [pharmaxis, ltd, pxs, ax, says, is, seeking, t...\n",
       "5637              [diary, top, economic, events, to, jun]\n",
       "5640                 [seoul, shares, ks, open, down, pct]\n",
       "5650    [brief, australia, pharmaxis, says, fda, inclu...\n",
       "5652    [diary, emerging, markets, economic, events, t...\n",
       "5656    [shares, in, australia, cochlear, coh, ax, fal...\n",
       "6019          [boj, balance, market, operations, revised]\n",
       "6022              [taiwan, stocks, twii, open, down, pct]\n",
       "6027    [soccer, midfielder, gago, added, to, argentin...\n",
       "6032    [north, american, power, transmission, outage,...\n",
       "6565    [tennis, nadal, looks, for, renewed, vigour, a...\n",
       "6568                    [taiwan, may, pmi, vs, in, april]\n",
       "6570                    [taiwan, may, pmi, vs, in, april]\n",
       "6580                    [taiwan, pmi, slips, to, in, may]\n",
       "6589                        [top, news, asian, companies]\n",
       "7059    [diary, stoxx, europe, major, companies, resul...\n",
       "7062    [diary, uk, and, irish, corporate, events, fro...\n",
       "7095       [table, japan, exchange, group, div, forecast]\n",
       "7101           [technicals, lme, aluminium, to, drop, to]\n",
       "7104       [keywords, commodities, technicals, aluminium]\n",
       "7105    [service, alert, thomson, reuters, dealing, em...\n",
       "7112                  [reuters, insider, upcoming, shows]\n",
       "7658                       [india, call, money, pct, jun]\n",
       "7666    [illinois, credit, concerns, rise, after, fail...\n",
       "7678    [buzz, china, mfg, pmis, confirm, slowdown, ri...\n",
       "Name: TAKE_TEXT, Length: 76, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list[0] \n",
    "# Not as clean as I would ideatlly like, \n",
    "# But similar document do get clustered together (e.g. market/economy and sports). We may further fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the \"Names\" in the largest topic using TF-IDF\n",
    "#### Here alternatively we can retreive most frequent words that are in the largest topic but not the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(docs):\n",
    "    \n",
    "    tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')   \n",
    "\n",
    "    X_train = list(map(lambda x: ' '.join(word for word in x), docs))\n",
    "\n",
    "    vect = tfidf.fit(X_train)\n",
    "\n",
    "    f = np.array(vect.get_feature_names())\n",
    "\n",
    "    X_train_transformed = vect.transform(X_train)\n",
    "\n",
    "    sorted_tfidf_index = X_train_transformed.max(0).toarray()[0].argsort()\n",
    "\n",
    "    sorted_tfidf_value = X_train_transformed.max(0).toarray()[0,sorted_tfidf_index]\n",
    "\n",
    "    df = pd.DataFrame(data =f[sorted_tfidf_index], columns=['vocab'])\n",
    "\n",
    "    df['tfidf_value'] = np.array(sorted_tfidf_value)\n",
    "\n",
    "    df = df.sort_values(['tfidf_value','vocab'], ascending =[False, True])\n",
    "    \n",
    "    df = df.iloc[0:10, ]\n",
    "\n",
    "    #l = pd.DataFrame(data = np.array(df['tfidf_value']), index = np.array(df['vocab']))[0:10]\n",
    "    \n",
    "    return df\n",
    "\n",
    "result = [get_top_words(x) for x in topic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            vocab  tfidf_value\n",
       " 315       vietnam     0.807446\n",
       " 314          news     0.731672\n",
       " 313  motorcycling     0.707107\n",
       " 312          page     0.681657\n",
       " 311         asian     0.671638\n",
       " 310         slips     0.669932\n",
       " 309      baseball     0.643279\n",
       " 307       banking     0.633383\n",
       " 308    investment     0.633383\n",
       " 306          june     0.631230,             vocab  tfidf_value\n",
       " 149          news     0.754908\n",
       " 148  motorcycling     0.680062\n",
       " 147          page     0.655831\n",
       " 144         asian     0.636477\n",
       " 143     companies     0.636477\n",
       " 146      emerging     0.636477\n",
       " 145       markets     0.636477\n",
       " 142         alert     0.601936\n",
       " 141        greece     0.520439\n",
       " 139         amers     0.512572,            vocab  tfidf_value\n",
       " 81        sports     0.859619\n",
       " 80   commodities     0.807292\n",
       " 79  motorcycling     0.730529\n",
       " 78          news     0.724205\n",
       " 77      baseball     0.696383\n",
       " 76          page     0.689585\n",
       " 75  agricultural     0.662451\n",
       " 74         alert     0.571874\n",
       " 73       service     0.554836\n",
       " 72     foodgrain     0.548642,        vocab  tfidf_value\n",
       " 253    offer     0.684799\n",
       " 252  bangkok     0.622288\n",
       " 251   ignore     0.616349\n",
       " 250   ignroe     0.616349\n",
       " 249       si     0.615016\n",
       " 247      pls     0.556828\n",
       " 248  testing     0.556828\n",
       " 246      pct     0.525956\n",
       " 245    alert     0.508762\n",
       " 244  factors     0.485515,            vocab  tfidf_value\n",
       " 54          news     0.774774\n",
       " 53        sports     0.763965\n",
       " 52  motorcycling     0.694231\n",
       " 51          page     0.632239\n",
       " 49      baseball     0.596416\n",
       " 50        update     0.596416\n",
       " 48         alert     0.592035\n",
       " 47       service     0.555262\n",
       " 46       results     0.537192\n",
       " 45          svcs     0.447530,          vocab  tfidf_value\n",
       " 221      alert     0.616889\n",
       " 218  companies     0.595986\n",
       " 219   european     0.595986\n",
       " 217  financial     0.595986\n",
       " 220   services     0.595986\n",
       " 216        aud     0.547202\n",
       " 215       news     0.538147\n",
       " 214    mideast     0.532688\n",
       " 213   malaysia     0.523872\n",
       " 212   thailand     0.523872,          vocab  tfidf_value\n",
       " 318       test     0.784347\n",
       " 317        qtr     0.670229\n",
       " 316        mth     0.612986\n",
       " 315      alert     0.607864\n",
       " 313  financial     0.607257\n",
       " 314   services     0.607257\n",
       " 312   exchange     0.591077\n",
       " 311    foreign     0.591077\n",
       " 310      money     0.590636\n",
       " 309       news     0.548868,            vocab  tfidf_value\n",
       " 45   commodities     0.744421\n",
       " 44          news     0.739278\n",
       " 42      baseball     0.707107\n",
       " 43       results     0.707107\n",
       " 41          page     0.673400\n",
       " 40  agricultural     0.648769\n",
       " 39         alert     0.563971\n",
       " 38        update     0.541494\n",
       " 37           nhc     0.500000\n",
       " 35       outlook     0.500000,           vocab  tfidf_value\n",
       " 133          ks     0.810514\n",
       " 132  california     0.672686\n",
       " 131       alert     0.652307\n",
       " 129      ignore     0.596955\n",
       " 130        test     0.596955\n",
       " 128       tokyo     0.535993\n",
       " 127     ontario     0.512606\n",
       " 123      battle     0.470196\n",
       " 126     federer     0.470196\n",
       " 124     reaches     0.470196,           vocab  tfidf_value\n",
       " 181    emerging     0.664477\n",
       " 182         ifr     0.664477\n",
       " 177       asian     0.639051\n",
       " 176     banking     0.639051\n",
       " 175   companies     0.639051\n",
       " 179       fixed     0.639051\n",
       " 178      income     0.639051\n",
       " 180  investment     0.639051\n",
       " 174     markets     0.600309\n",
       " 173       metal     0.568766,          vocab  tfidf_value\n",
       " 149        iss     0.707107\n",
       " 147       news     0.707107\n",
       " 148       page     0.707107\n",
       " 146      alert     0.656643\n",
       " 144        ifr     0.619579\n",
       " 145    markets     0.619579\n",
       " 143    baltics     0.608761\n",
       " 142      diary     0.608761\n",
       " 141         bo     0.595736\n",
       " 140  summaries     0.576645,          vocab  tfidf_value\n",
       " 111       news     0.768459\n",
       " 109     ignore     0.707107\n",
       " 110       test     0.707107\n",
       " 108    foreign     0.706783\n",
       " 107      china     0.656192\n",
       " 106    greater     0.656192\n",
       " 105       page     0.639899\n",
       " 104      alert     0.603723\n",
       " 103   exchange     0.582582\n",
       " 102  summaries     0.574754,         vocab  tfidf_value\n",
       " 61   baseball     0.869354\n",
       " 60     metals     0.843988\n",
       " 59       news     0.707107\n",
       " 58       page     0.707107\n",
       " 57    finnish     0.675024\n",
       " 56  brazilian     0.675024\n",
       " 55        soy     0.629606\n",
       " 54    scorers     0.623062\n",
       " 53  summaries     0.623062\n",
       " 52     kuwait     0.620615,             vocab  tfidf_value\n",
       " 277          test     0.795527\n",
       " 274           boj     0.512276\n",
       " 276          rate     0.512276\n",
       " 275       revised     0.512276\n",
       " 273      mahindra     0.500144\n",
       " 272        digest     0.490644\n",
       " 271         press     0.490644\n",
       " 269  certificates     0.471465\n",
       " 270         dealt     0.471465\n",
       " 268       deposit     0.471465]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news</td>\n",
       "      <td>sports</td>\n",
       "      <td>offer</td>\n",
       "      <td>news</td>\n",
       "      <td>alert</td>\n",
       "      <td>test</td>\n",
       "      <td>commodities</td>\n",
       "      <td>ks</td>\n",
       "      <td>emerging</td>\n",
       "      <td>iss</td>\n",
       "      <td>news</td>\n",
       "      <td>baseball</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>motorcycling</td>\n",
       "      <td>commodities</td>\n",
       "      <td>bangkok</td>\n",
       "      <td>sports</td>\n",
       "      <td>companies</td>\n",
       "      <td>qtr</td>\n",
       "      <td>news</td>\n",
       "      <td>california</td>\n",
       "      <td>ifr</td>\n",
       "      <td>news</td>\n",
       "      <td>ignore</td>\n",
       "      <td>metals</td>\n",
       "      <td>boj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>page</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>ignore</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>european</td>\n",
       "      <td>mth</td>\n",
       "      <td>baseball</td>\n",
       "      <td>alert</td>\n",
       "      <td>asian</td>\n",
       "      <td>page</td>\n",
       "      <td>test</td>\n",
       "      <td>news</td>\n",
       "      <td>rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asian</td>\n",
       "      <td>news</td>\n",
       "      <td>ignroe</td>\n",
       "      <td>page</td>\n",
       "      <td>financial</td>\n",
       "      <td>alert</td>\n",
       "      <td>results</td>\n",
       "      <td>ignore</td>\n",
       "      <td>banking</td>\n",
       "      <td>alert</td>\n",
       "      <td>foreign</td>\n",
       "      <td>page</td>\n",
       "      <td>revised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>companies</td>\n",
       "      <td>baseball</td>\n",
       "      <td>si</td>\n",
       "      <td>baseball</td>\n",
       "      <td>services</td>\n",
       "      <td>financial</td>\n",
       "      <td>page</td>\n",
       "      <td>test</td>\n",
       "      <td>companies</td>\n",
       "      <td>ifr</td>\n",
       "      <td>china</td>\n",
       "      <td>finnish</td>\n",
       "      <td>mahindra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emerging</td>\n",
       "      <td>page</td>\n",
       "      <td>pls</td>\n",
       "      <td>update</td>\n",
       "      <td>aud</td>\n",
       "      <td>services</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>tokyo</td>\n",
       "      <td>fixed</td>\n",
       "      <td>markets</td>\n",
       "      <td>greater</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>digest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>markets</td>\n",
       "      <td>agricultural</td>\n",
       "      <td>testing</td>\n",
       "      <td>alert</td>\n",
       "      <td>news</td>\n",
       "      <td>exchange</td>\n",
       "      <td>alert</td>\n",
       "      <td>ontario</td>\n",
       "      <td>income</td>\n",
       "      <td>baltics</td>\n",
       "      <td>page</td>\n",
       "      <td>soy</td>\n",
       "      <td>press</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alert</td>\n",
       "      <td>alert</td>\n",
       "      <td>pct</td>\n",
       "      <td>service</td>\n",
       "      <td>mideast</td>\n",
       "      <td>foreign</td>\n",
       "      <td>update</td>\n",
       "      <td>battle</td>\n",
       "      <td>investment</td>\n",
       "      <td>diary</td>\n",
       "      <td>alert</td>\n",
       "      <td>scorers</td>\n",
       "      <td>certificates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greece</td>\n",
       "      <td>service</td>\n",
       "      <td>alert</td>\n",
       "      <td>results</td>\n",
       "      <td>malaysia</td>\n",
       "      <td>money</td>\n",
       "      <td>nhc</td>\n",
       "      <td>federer</td>\n",
       "      <td>markets</td>\n",
       "      <td>bo</td>\n",
       "      <td>exchange</td>\n",
       "      <td>summaries</td>\n",
       "      <td>dealt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amers</td>\n",
       "      <td>foodgrain</td>\n",
       "      <td>factors</td>\n",
       "      <td>svcs</td>\n",
       "      <td>thailand</td>\n",
       "      <td>news</td>\n",
       "      <td>outlook</td>\n",
       "      <td>reaches</td>\n",
       "      <td>metal</td>\n",
       "      <td>summaries</td>\n",
       "      <td>summaries</td>\n",
       "      <td>kuwait</td>\n",
       "      <td>deposit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1             2        3             4          5          6   \\\n",
       "0          news        sports    offer          news      alert       test   \n",
       "1  motorcycling   commodities  bangkok        sports  companies        qtr   \n",
       "2          page  motorcycling   ignore  motorcycling   european        mth   \n",
       "3         asian          news   ignroe          page  financial      alert   \n",
       "4     companies      baseball       si      baseball   services  financial   \n",
       "5      emerging          page      pls        update        aud   services   \n",
       "6       markets  agricultural  testing         alert       news   exchange   \n",
       "7         alert         alert      pct       service    mideast    foreign   \n",
       "8        greece       service    alert       results   malaysia      money   \n",
       "9         amers     foodgrain  factors          svcs   thailand       news   \n",
       "\n",
       "             7           8           9          10         11         12  \\\n",
       "0   commodities          ks    emerging        iss       news   baseball   \n",
       "1          news  california         ifr       news     ignore     metals   \n",
       "2      baseball       alert       asian       page       test       news   \n",
       "3       results      ignore     banking      alert    foreign       page   \n",
       "4          page        test   companies        ifr      china    finnish   \n",
       "5  agricultural       tokyo       fixed    markets    greater  brazilian   \n",
       "6         alert     ontario      income    baltics       page        soy   \n",
       "7        update      battle  investment      diary      alert    scorers   \n",
       "8           nhc     federer     markets         bo   exchange  summaries   \n",
       "9       outlook     reaches       metal  summaries  summaries     kuwait   \n",
       "\n",
       "             13  \n",
       "0          test  \n",
       "1           boj  \n",
       "2          rate  \n",
       "3       revised  \n",
       "4      mahindra  \n",
       "5        digest  \n",
       "6         press  \n",
       "7  certificates  \n",
       "8         dealt  \n",
       "9       deposit  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame.from_dict({k : v['vocab'].tolist() for k, v in enumerate(result) if k !=0})\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
