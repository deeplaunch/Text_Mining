{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending topics\n",
    "\n",
    "#### While topic modeling such as LDA seems to be an easy tool to identify topics, my past experience using it showed three drawbacks when it comes to short and high-frequency content.\n",
    "\n",
    "1. Once an unsupervised model is selected, classfications become static, which means \"unseen\" topics in the future won't be able to be captured.\n",
    "\n",
    "2. Short messages are more likely to contain a homogenous topic than not, which challenges LDA's assumption of a probability distribution of \"multiple\" topics in each document.\n",
    "\n",
    "3. Ultimately LDA is a statistical model that isn't quite able capture similar semantics of words\n",
    "\n",
    "#### Instead, here I build a *\"live\"* pipeline of trending topic detector combining several techniques such as Doc2Vec and hierachical clustering. This approach allws us to focus on recent documents, regardless what the past topic distribution might look like.\n",
    "\n",
    "\n",
    "#### _Please note: here I don't have any API for news feed, thus using a file sent from a friend of mine. However the approach is similar._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import xlrd\n",
    "import matplotlib as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load and Clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8213, 19)\n",
      "Index(['DATE', 'TIME', 'UNIQUE_STORY_INDEX', 'EVENT_TYPE', 'PNAC',\n",
      "       'STORY_DATE_TIME', 'TAKE_DATE_TIME', 'HEADLINE_ALERT_TEXT',\n",
      "       'ACCUMULATED_STORY_TEXT', 'TAKE_TEXT', 'PRODUCTS', 'TOPICS',\n",
      "       'RELATED_RICS', 'NAMED_ITEMS', 'HEADLINE_SUBTYPE', 'STORY_TYPE',\n",
      "       'TABULAR_FLAG', 'ATTRIBUTION', 'LANGUAGE'],\n",
      "      dtype='object')\n",
      "(3137, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv('sample_file.csv') \n",
    "print(text_df.shape)\n",
    "print(text_df.columns)\n",
    "\n",
    "# remove rows that indicates \"please ignore\"\n",
    "text_df = text_df[text_df.HEADLINE_ALERT_TEXT.str.contains(\"Test, Please Ignore\")==False]\n",
    "text_df.LANGUAGE.value_counts()\n",
    "\n",
    "#### Only English\n",
    "text_df = text_df[text_df['LANGUAGE']=='EN']\n",
    "\n",
    "## Use Headline when no Take Text is available\n",
    "text_df.loc[text_df['TAKE_TEXT'].isnull(),'TAKE_TEXT'] = text_df.loc[text_df['TAKE_TEXT'].isnull(),'HEADLINE_ALERT_TEXT']\n",
    "\n",
    "## fill missing time\n",
    "text_df.DATE = text_df.DATE.fillna(method = 'ffill')\n",
    "text_df.TIME = text_df.TIME.fillna(method = 'ffill')\n",
    "\n",
    "## format time\n",
    "\n",
    "text_df.DATE = pd.to_datetime(text_df.DATE)\n",
    "text_df['HOUR'] = text_df.TIME.apply(lambda x: x.split(':')[0])\n",
    "\n",
    "# cleaning text\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.str.lower().str.strip().str.replace('[^\\w\\s]''',' ').str.replace('[^a-zA-Z0-9'' ]',' ').str.replace(r'\\W*\\b\\w{1,1}\\b', '')\n",
    "\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.apply(lambda x: re.sub(r'[\\*|\\+|\\_|\\-|\\<||>|\\(|\\)]','',x))\n",
    "\n",
    "text_df = text_df[text_df.TAKE_TEXT.notnull()]\n",
    "\n",
    "# apply gensim processing\n",
    "text_df['TAKE_TEXT'] = text_df['TAKE_TEXT'].apply(gensim.utils.simple_preprocess)\n",
    "print(text_df.shape)\n",
    "\n",
    "train_corpus = text_df.TAKE_TEXT.tolist()\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(value, [key])for key , value in enumerate(train_corpus)]\n",
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a Doc2Vec Model quickly (can also use a pre-trained model from wiki for transfer-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "# train model\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test of the model ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Document (2259): «boj balance market operations revised»\n",
      "\n",
      "Similar Document (133, 0.7650014758110046): «table breakdown of china may official pmi»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random check to see similar documents are indeed identified (cltr-end for a few examples)\n",
    "doc_id = np.random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Selected Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cluster documents for topic identification using Hierachical Agglomerative Clustering\n",
    "#### Here clustering is executed in each time window (4 hours as I choose). We are able to identify the top topic in each time window (my defiintion of \"trending\"). Such \"onlineness\" is a key feature of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def find_largest_topic(vector):\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters= 120, linkage='ward')  # Here we want many clusters to get smaller ones\n",
    "    cluster.fit_predict(vector)\n",
    "    max_id = most_common(list(cluster.labels_))\n",
    "    result_id = (cluster.labels_ == max_id)\n",
    "    \n",
    "    return result_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['vector'] = [model.infer_vector(x.words) for x in train_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break data into 4-hour window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = text_df\n",
    "result_df.HOUR = result_df.HOUR.apply(float)\n",
    "n = int(result_df.HOUR.max() / 4)\n",
    "result_df['Hour_4'] = pd.qcut(result_df.HOUR, n , labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list =[]\n",
    "\n",
    "for x in range(n):\n",
    "    vector = result_df.loc[result_df.Hour_4 == x, 'vector'].tolist()\n",
    "    top_topic_id = find_largest_topic(vector)\n",
    "    top_docs = result_df.loc[result_df.Hour_4 == x, 'TAKE_TEXT'][top_topic_id]\n",
    "    topic_list.append(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468     [peru, inflation, index, pecpi, eci, rises, pc...\n",
       "561                        [india, call, money, pct, jun]\n",
       "609     [exxon, mobil, xom, reports, unplanned, flarin...\n",
       "610     [exxon, reports, unplanned, flaring, breakdown...\n",
       "613                           [update, baseball, results]\n",
       "991     [service, alert, thomson, reuters, dealing, re...\n",
       "1108                             [top, news, front, page]\n",
       "1942                                    [emea, test, rcf]\n",
       "1943    [by, ramkumar, koottala, hari, abcdefghijklmn,...\n",
       "2224                             [top, news, front, page]\n",
       "2227                             [top, news, front, page]\n",
       "2467                             [top, news, front, page]\n",
       "3331    [motorcycling, motorcycling, grand, prix, moto...\n",
       "4395    [service, alert, datascope, equities, planned,...\n",
       "5602                                     [diary, vietnam]\n",
       "5637              [diary, top, economic, events, to, jun]\n",
       "5640                 [seoul, shares, ks, open, down, pct]\n",
       "5652    [diary, emerging, markets, economic, events, t...\n",
       "6022              [taiwan, stocks, twii, open, down, pct]\n",
       "6032    [north, american, power, transmission, outage,...\n",
       "6568                    [taiwan, may, pmi, vs, in, april]\n",
       "6570                    [taiwan, may, pmi, vs, in, april]\n",
       "6580                    [taiwan, pmi, slips, to, in, may]\n",
       "7062    [diary, uk, and, irish, corporate, events, fro...\n",
       "7101           [technicals, lme, aluminium, to, drop, to]\n",
       "7104       [keywords, commodities, technicals, aluminium]\n",
       "7105    [service, alert, thomson, reuters, dealing, em...\n",
       "7112                  [reuters, insider, upcoming, shows]\n",
       "7646                [italy, factors, to, watch, on, june]\n",
       "7658                       [india, call, money, pct, jun]\n",
       "Name: TAKE_TEXT, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list[0] \n",
    "# Not as clean as I would ideatlly like, \n",
    "# But similar document do get clustered together (e.g. market/economy and sports). We may further fine-tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the \"Names\" in the largest topic using TF-IDF\n",
    "#### Here alternatively we can retreive most frequent words that are in the largest topic but not the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a tfidf on entire corpus\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')   \n",
    "\n",
    "X_train = list(map(lambda x: ' '.join(word for word in x), result_df.TAKE_TEXT))\n",
    "\n",
    "vect = tfidf.fit(X_train)\n",
    "\n",
    "feature = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to each topic list to get most distinct words withitn the topic\n",
    "\n",
    "def get_top_words(docs):\n",
    "    \n",
    "    docs = list(map(lambda x: ' '.join(word for word in x), docs))\n",
    "    \n",
    "    X_train_transformed = vect.transform(docs)\n",
    "\n",
    "    sorted_tfidf_index = X_train_transformed.max(0).toarray()[0].argsort()\n",
    "\n",
    "    sorted_tfidf_value = X_train_transformed.max(0).toarray()[0,sorted_tfidf_index]\n",
    "\n",
    "    df = pd.DataFrame(data =feature[sorted_tfidf_index], columns=['vocab'])\n",
    "\n",
    "    df['tfidf_value'] = np.array(sorted_tfidf_value)\n",
    "\n",
    "    df = df.sort_values(['tfidf_value','vocab'], ascending =[False, True])\n",
    "    \n",
    "    df = df.iloc[0:10, ]\n",
    "\n",
    "    #l = pd.DataFrame(data = np.array(df['tfidf_value']), index = np.array(df['vocab']))[0:10]\n",
    "    \n",
    "    return df\n",
    "\n",
    "result = [get_top_words(x) for x in topic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>motorcycling</td>\n",
       "      <td>sports</td>\n",
       "      <td>baseball</td>\n",
       "      <td>page</td>\n",
       "      <td>failover</td>\n",
       "      <td>sports</td>\n",
       "      <td>page</td>\n",
       "      <td>tops</td>\n",
       "      <td>generates</td>\n",
       "      <td>sports</td>\n",
       "      <td>ignore</td>\n",
       "      <td>metals</td>\n",
       "      <td>brief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rcf</td>\n",
       "      <td>page</td>\n",
       "      <td>malaysia</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>services</td>\n",
       "      <td>commodities</td>\n",
       "      <td>news</td>\n",
       "      <td>ontario</td>\n",
       "      <td>management</td>\n",
       "      <td>page</td>\n",
       "      <td>greater</td>\n",
       "      <td>baseball</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alert</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>page</td>\n",
       "      <td>baseball</td>\n",
       "      <td>alberta</td>\n",
       "      <td>page</td>\n",
       "      <td>gc</td>\n",
       "      <td>california</td>\n",
       "      <td>winners</td>\n",
       "      <td>ifr</td>\n",
       "      <td>hungarian</td>\n",
       "      <td>page</td>\n",
       "      <td>ramkumar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amers</td>\n",
       "      <td>baseball</td>\n",
       "      <td>pls</td>\n",
       "      <td>news</td>\n",
       "      <td>companies</td>\n",
       "      <td>lynx</td>\n",
       "      <td>warehouse</td>\n",
       "      <td>outperforms</td>\n",
       "      <td>carbon</td>\n",
       "      <td>news</td>\n",
       "      <td>exchange</td>\n",
       "      <td>standings</td>\n",
       "      <td>mahindra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hdcp</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "      <td>alert</td>\n",
       "      <td>european</td>\n",
       "      <td>baseball</td>\n",
       "      <td>hg</td>\n",
       "      <td>nzs</td>\n",
       "      <td>slaughterhouse</td>\n",
       "      <td>upcoming</td>\n",
       "      <td>foreign</td>\n",
       "      <td>soy</td>\n",
       "      <td>finland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>constructor</td>\n",
       "      <td>polish</td>\n",
       "      <td>diary</td>\n",
       "      <td>results</td>\n",
       "      <td>australia</td>\n",
       "      <td>news</td>\n",
       "      <td>benitec</td>\n",
       "      <td>aej</td>\n",
       "      <td>nzs</td>\n",
       "      <td>insider</td>\n",
       "      <td>bajaj</td>\n",
       "      <td>collated</td>\n",
       "      <td>hari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>semifinal</td>\n",
       "      <td>nhc</td>\n",
       "      <td>ignroe</td>\n",
       "      <td>update</td>\n",
       "      <td>hungarian</td>\n",
       "      <td>mth</td>\n",
       "      <td>biopharma</td>\n",
       "      <td>alert</td>\n",
       "      <td>kills</td>\n",
       "      <td>scorers</td>\n",
       "      <td>offs</td>\n",
       "      <td>news</td>\n",
       "      <td>digest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>argentine</td>\n",
       "      <td>tropical</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>service</td>\n",
       "      <td>financial</td>\n",
       "      <td>hungarian</td>\n",
       "      <td>blt</td>\n",
       "      <td>ringgit</td>\n",
       "      <td>gateway</td>\n",
       "      <td>markets</td>\n",
       "      <td>swedish</td>\n",
       "      <td>finnish</td>\n",
       "      <td>alert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>datascope</td>\n",
       "      <td>argentine</td>\n",
       "      <td>offs</td>\n",
       "      <td>unavailable</td>\n",
       "      <td>alert</td>\n",
       "      <td>upcoming</td>\n",
       "      <td>alert</td>\n",
       "      <td>redbank</td>\n",
       "      <td>tourist</td>\n",
       "      <td>liga</td>\n",
       "      <td>test</td>\n",
       "      <td>stl</td>\n",
       "      <td>corporate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>delhi</td>\n",
       "      <td>freight</td>\n",
       "      <td>scorers</td>\n",
       "      <td>washout</td>\n",
       "      <td>enoc</td>\n",
       "      <td>finnish</td>\n",
       "      <td>pjm</td>\n",
       "      <td>lin</td>\n",
       "      <td>coal</td>\n",
       "      <td>alert</td>\n",
       "      <td>china</td>\n",
       "      <td>hungarian</td>\n",
       "      <td>factors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1             2             3             4          5   \\\n",
       "0  motorcycling        sports      baseball          page   failover   \n",
       "1           rcf          page      malaysia  motorcycling   services   \n",
       "2         alert  motorcycling          page      baseball    alberta   \n",
       "3         amers      baseball           pls          news  companies   \n",
       "4          hdcp          news          news         alert   european   \n",
       "5   constructor        polish         diary       results  australia   \n",
       "6     semifinal           nhc        ignroe        update  hungarian   \n",
       "7     argentine      tropical  motorcycling       service  financial   \n",
       "8     datascope     argentine          offs   unavailable      alert   \n",
       "9         delhi       freight       scorers       washout       enoc   \n",
       "\n",
       "            6          7            8               9         10         11  \\\n",
       "0       sports       page         tops       generates    sports     ignore   \n",
       "1  commodities       news      ontario      management      page    greater   \n",
       "2         page         gc   california         winners       ifr  hungarian   \n",
       "3         lynx  warehouse  outperforms          carbon      news   exchange   \n",
       "4     baseball         hg          nzs  slaughterhouse  upcoming    foreign   \n",
       "5         news    benitec          aej             nzs   insider      bajaj   \n",
       "6          mth  biopharma        alert           kills   scorers       offs   \n",
       "7    hungarian        blt      ringgit         gateway   markets    swedish   \n",
       "8     upcoming      alert      redbank         tourist      liga       test   \n",
       "9      finnish        pjm          lin            coal     alert      china   \n",
       "\n",
       "          12         13  \n",
       "0     metals      brief  \n",
       "1   baseball       test  \n",
       "2       page   ramkumar  \n",
       "3  standings   mahindra  \n",
       "4        soy    finland  \n",
       "5   collated       hari  \n",
       "6       news     digest  \n",
       "7    finnish      alert  \n",
       "8        stl  corporate  \n",
       "9  hungarian    factors  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to dataframe\n",
    "result = pd.DataFrame.from_dict({k : v['vocab'].tolist() for k, v in enumerate(result) if k !=0})\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
