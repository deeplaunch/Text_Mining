{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending topics\n",
    "\n",
    "#### While topic modeling such as LDA seems to be an easy tool to identify topics, my past experience using it showed three drawbacks when it comes to short and high-frequency content.\n",
    "\n",
    "1. Once an unsupervised model is selected, classfications become static, which means \"unseen\" topics in the future won't be able to be captured.\n",
    "\n",
    "2. Short messages are more likely to contain a homogenous topic than not, which challenges LDA's assumption of a probability distribution of \"multiple\" topics in each document.\n",
    "\n",
    "3. Ultimately LDA is a statistical model that isn't quite able capture similar semantics of words\n",
    "\n",
    "#### Instead, here I build a *\"live\"* pipeline of trending topic detector combining several techniques such as Doc2Vec and hierachical clustering. This approach allws us to focus on recent documents, regardless what the past topic distribution might look like.\n",
    "\n",
    "\n",
    "#### _Please note: here I don't have any API for news feed, thus using a file sent from a friend of mine. However the approach is similar._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import xlrd\n",
    "import matplotlib as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load and Clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8213, 19)\n",
      "Index(['DATE', 'TIME', 'UNIQUE_STORY_INDEX', 'EVENT_TYPE', 'PNAC',\n",
      "       'STORY_DATE_TIME', 'TAKE_DATE_TIME', 'HEADLINE_ALERT_TEXT',\n",
      "       'ACCUMULATED_STORY_TEXT', 'TAKE_TEXT', 'PRODUCTS', 'TOPICS',\n",
      "       'RELATED_RICS', 'NAMED_ITEMS', 'HEADLINE_SUBTYPE', 'STORY_TYPE',\n",
      "       'TABULAR_FLAG', 'ATTRIBUTION', 'LANGUAGE'],\n",
      "      dtype='object')\n",
      "(3137, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3137"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.read_csv('sample_file.csv') \n",
    "print(text_df.shape)\n",
    "print(text_df.columns)\n",
    "\n",
    "# remove rows that indicates \"please ignore\"\n",
    "text_df = text_df[text_df.HEADLINE_ALERT_TEXT.str.contains(\"Test, Please Ignore\")==False]\n",
    "text_df.LANGUAGE.value_counts()\n",
    "\n",
    "#### Only English\n",
    "text_df = text_df[text_df['LANGUAGE']=='EN']\n",
    "\n",
    "## Use Headline when no Take Text is available\n",
    "text_df.loc[text_df['TAKE_TEXT'].isnull(),'TAKE_TEXT'] = text_df.loc[text_df['TAKE_TEXT'].isnull(),'HEADLINE_ALERT_TEXT']\n",
    "\n",
    "## fill missing time\n",
    "text_df.DATE = text_df.DATE.fillna(method = 'ffill')\n",
    "text_df.TIME = text_df.TIME.fillna(method = 'ffill')\n",
    "\n",
    "## format time\n",
    "\n",
    "text_df.DATE = pd.to_datetime(text_df.DATE)\n",
    "text_df['HOUR'] = text_df.TIME.apply(lambda x: x.split(':')[0])\n",
    "\n",
    "# cleaning text\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.str.lower().str.strip().str.replace('[^\\w\\s]''',' ').str.replace('[^a-zA-Z0-9'' ]',' ').str.replace(r'\\W*\\b\\w{1,1}\\b', '')\n",
    "\n",
    "text_df.TAKE_TEXT = text_df.TAKE_TEXT.apply(lambda x: re.sub(r'[\\*|\\+|\\_|\\-|\\<||>|\\(|\\)]','',x))\n",
    "\n",
    "text_df = text_df[text_df.TAKE_TEXT.notnull()]\n",
    "\n",
    "# apply gensim processing\n",
    "text_df['TAKE_TEXT'] = text_df['TAKE_TEXT'].apply(gensim.utils.simple_preprocess)\n",
    "print(text_df.shape)\n",
    "\n",
    "train_corpus = text_df.TAKE_TEXT.tolist()\n",
    "train_corpus = [gensim.models.doc2vec.TaggedDocument(value, [key])for key , value in enumerate(train_corpus)]\n",
    "len(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train a Doc2Vec Model quickly (can also use a pre-trained model from wiki for transfer-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(train_corpus)\n",
    "\n",
    "# train model\n",
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    second_ranks.append(sims[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some test of the model ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Document (1277): «moscow cairo june reuters russian firm altimo said on sunday it had no plans to launch new offer to buy out minority shareholders in egypt orascom telecom orte ca ot after the egyptian regulator rejected its attempt to reopen its earlier tender orascom telecom heavyweight on the egyptian stock exchange is already percent owned by russia vimpelcom vip which in turn is percent owned by altimo altimo launched an offer in april to buy out minority shareholders of orascom which has mobile businesses in canada algeria pakistan and other emerging markets such as bangladesh in deal worth an estimated billion but altimo failed last week to secure sufficient acceptances from the minority shareholders in orascom by monday deadline after shareholders with only percent of orascom telecom shares listed on the egyptian stock exchange offered to sell shares below the minimum percent required for the buy out to go ahead at the moment there are no plans to launch new offer altimo vice president evgeny dumalkin said on sunday the egyptian regulator didn give us the waiver to re open the tender and permitted only withdrawals so we couldn proceed with the offer though we were willing to he said reporting by maria kiselyova in moscow and patrick werr in cairo editing by greg mahlich patrick werr thomsonreuters com reuters messaging patrick werr thomsonreuters com thomsonreuters net keywords altimo orascom»\n",
      "\n",
      "Similar Document (1307, 0.8773731589317322): «adds background moscow cairo june reuters russian firm altimo said on sunday it had no plans to launch new offer to buy out minority shareholders in orascom telecom oth orte ca after egypt regulator rejected its attempt to reopen its earlier tender orascom telecom heavyweight on the egyptian stock exchange is percent owned by vimpelcom vip which in turn is percent owned by altimo altimo launched an offer in april to buy out minority shareholders of orascom which has mobile businesses in canada algeria pakistan and other markets such as bangladesh in deal worth an estimated billion but it failed to secure sufficient acceptances from oth minorities by deadline last monday after shareholders with only percent of the shares offered to sell below minimum percent required altimo then asked the egyptian financial supervisory authority efsa to waive the minimum take up requirement for minority shareholders tendering their stock but efsa turned down the request efsa didn give us the waiver to reopen tender and permitted only withdrawals so we couldn proceed with the offer though we were willing to altimo vice president evgeny dumalkin said on sunday at the moment there are no plans to launch new offer he said oth shares were trading percent lower on the egyptian stock exchange at gmt sources had said that vimpelcom planned to delist oth to lower costs by closing down its egyptian office and improve efficiency by placing oth assets under vimpelcom direct management id nl yc reporting by maria kiselyova in moscow and patrick werr in cairo editing by greg mahlich and jason neely patrick werr thomsonreuters com reuters messaging patrick werr thomsonreuters com thomsonreuters net keywords altimo orascom»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random check to see similar documents are indeed identified (cltr-end for a few examples)\n",
    "doc_id = np.random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Selected Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cluster documents for topic identification using Hierachical Agglomerative Clustering\n",
    "#### Here clustering is executed in each time window (4 hours as I choose). We are able to identify the top topic in each time window (my defiintion of \"trending\"). Such \"onlineness\" is a key feature of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(ls):\n",
    "    return max(set(ls), key=ls.count)\n",
    "\n",
    "def find_largest_topic(vector):\n",
    "    \n",
    "    cluster = AgglomerativeClustering(n_clusters= 140, linkage='ward')  # Here we want many clusters to get smaller ones\n",
    "    cluster.fit_predict(vector)\n",
    "    max_id = most_common(list(cluster.labels_))\n",
    "    result_id = (cluster.labels_ == max_id)\n",
    "    \n",
    "    return result_id \n",
    "\n",
    "text_df['vector'] = [model.infer_vector(x.words) for x in train_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break data into 4-hour window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = text_df\n",
    "result_df.HOUR = result_df.HOUR.apply(float)\n",
    "n = int(result_df.HOUR.max() / 4)\n",
    "result_df['Hour_4'] = pd.qcut(result_df.HOUR, n , labels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list =[]\n",
    "\n",
    "for x in range(n):\n",
    "    vector = result_df.loc[result_df.Hour_4 == x, 'vector'].tolist()\n",
    "    top_topic_id = find_largest_topic(vector)\n",
    "    top_docs = result_df.loc[result_df.Hour_4 == x, 'TAKE_TEXT'][top_topic_id]\n",
    "    topic_list.append(top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25                       [top, news, investment, banking]\n",
       "153     [china, may, official, pmi, at, reuters, poll,...\n",
       "485     [corrected, brief, mitsubishi, to, buy, los, g...\n",
       "488     [corrected, mitsubishi, to, buy, grain, compan...\n",
       "1114    [tennis, french, open, men, singles, round, re...\n",
       "1236    [service, alert, burgundy, exchange, informati...\n",
       "2098    [leading, sunni, muslim, cleric, calls, for, j...\n",
       "2670    [feature, bombs, boredom, threaten, pakistan, ...\n",
       "2907                   [inside, middle, east, debt, june]\n",
       "3107    [rpt, mideast, debt, issuers, warm, to, cross,...\n",
       "3120    [soccer, world, cup, asia, qualifying, fixture...\n",
       "3423    [rpt, cyber, hacking, to, overshadow, summit, ...\n",
       "3431    [tennis, french, open, men, singles, round, re...\n",
       "3578    [rallying, latvala, wins, acropolis, rally, fo...\n",
       "4505    [bis, lays, out, simple, plan, for, how, to, h...\n",
       "4948              [column, the, economics, of, austerity]\n",
       "4962    [golf, kuchar, wins, memorial, title, by, two,...\n",
       "5270    [british, manufacturers, urge, health, budget,...\n",
       "5272    [brief, national, australia, bank, plans, mln,...\n",
       "5607      [korea, may, manufacturing, pmi, vs, in, april]\n",
       "5609    [factbox, speakers, at, reuters, wealth, manag...\n",
       "5616               [buzz, eur, aud, uptrend, strong, for]\n",
       "5622    [japan, corporate, capital, spending, falls, p...\n",
       "5650    [brief, australia, pharmaxis, says, fda, inclu...\n",
       "6018    [china, may, official, non, manufacturing, pmi...\n",
       "6030    [china, may, official, services, pmi, at, vs, ...\n",
       "6565    [tennis, nadal, looks, for, renewed, vigour, a...\n",
       "6589                        [top, news, asian, companies]\n",
       "7060    [table, bangladesh, key, economic, indicators,...\n",
       "7087    [indonesia, hsbc, may, manufacturing, pmi, at,...\n",
       "7678    [buzz, china, mfg, pmis, confirm, slowdown, ri...\n",
       "Name: TAKE_TEXT, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not as clean as I would ideatlly like, \n",
    "# But similar document do get clustered together (e.g. market/economy and sports). \n",
    "# We may further fine-tune it.\n",
    "topic_list[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identify the \"Names\" in the largest topic using TF-IDF\n",
    "#### Here alternatively we can retreive most frequent words that are in the largest topic but not the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'abroad', ..., 'zb', 'zealand', 'zone'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a tfidf on entire corpus\n",
    "tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', min_df= 18, max_df=0.035)   \n",
    "X_train = list(map(lambda x: ' '.join(word for word in x), result_df.TAKE_TEXT))\n",
    "vect = tfidf.fit(X_train)\n",
    "feature = np.array(vect.get_feature_names())\n",
    "print(len(feature))\n",
    "\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['austerity', 'grain', 'slowdown', 'boredom', 'nishikori',\n",
       "       'economics', 'sunni', 'includes', 'pmis', 'buy', 'buzz', 'men',\n",
       "       'singles', 'los', 'wins', 'summit', 'rpt', 'manufacturing',\n",
       "       'brief', 'vs'], dtype='<U15')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exaample\n",
    "test = [x for y in topic_list[0] for x in y ]\n",
    "test = ' '.join(test)\n",
    "test\n",
    "x = tfidf.transform([test])\n",
    "y = x.toarray()[0].argsort()[-20:]\n",
    "y\n",
    "\n",
    "feature[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to each topic list to get most distinct words withitn the topic\n",
    "def get_top_words(docs):\n",
    "    \n",
    "    # merge top topic into one document for tf-idf performance\n",
    "    docs = [x for y in docs for x in y ] \n",
    "    docs =[' '.join(docs)]\n",
    "    \n",
    "    # get tf-idf and sort words by returned value\n",
    "    tfidf = vect.transform(docs)\n",
    "    sorted_tfidf_index = tfidf.toarray()[0].argsort()[-15:]\n",
    "    df = pd.DataFrame(data = feature[sorted_tfidf_index], columns=['vocab'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "result = [get_top_words(x) for x in topic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>economics</td>\n",
       "      <td>mail</td>\n",
       "      <td>prix</td>\n",
       "      <td>ric</td>\n",
       "      <td>forced</td>\n",
       "      <td>weather</td>\n",
       "      <td>unchanged</td>\n",
       "      <td>forex</td>\n",
       "      <td>downgrade</td>\n",
       "      <td>digest</td>\n",
       "      <td>rdf</td>\n",
       "      <td>forecasts</td>\n",
       "      <td>rdf</td>\n",
       "      <td>prix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunni</td>\n",
       "      <td>rdf</td>\n",
       "      <td>dealing</td>\n",
       "      <td>svcs</td>\n",
       "      <td>force</td>\n",
       "      <td>baseball</td>\n",
       "      <td>nzd</td>\n",
       "      <td>form</td>\n",
       "      <td>subdebt</td>\n",
       "      <td>dealing</td>\n",
       "      <td>ignore</td>\n",
       "      <td>forex</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>created</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>includes</td>\n",
       "      <td>dealing</td>\n",
       "      <td>rs</td>\n",
       "      <td>moody</td>\n",
       "      <td>foot</td>\n",
       "      <td>standings</td>\n",
       "      <td>buzz</td>\n",
       "      <td>forum</td>\n",
       "      <td>reviews</td>\n",
       "      <td>block</td>\n",
       "      <td>nhl</td>\n",
       "      <td>form</td>\n",
       "      <td>charges</td>\n",
       "      <td>rdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pmis</td>\n",
       "      <td>svcs</td>\n",
       "      <td>resolved</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>hands</td>\n",
       "      <td>championship</td>\n",
       "      <td>man</td>\n",
       "      <td>friendly</td>\n",
       "      <td>usd</td>\n",
       "      <td>ric</td>\n",
       "      <td>message</td>\n",
       "      <td>forum</td>\n",
       "      <td>message</td>\n",
       "      <td>ignore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buy</td>\n",
       "      <td>rating</td>\n",
       "      <td>russia</td>\n",
       "      <td>ratings</td>\n",
       "      <td>founder</td>\n",
       "      <td>diary</td>\n",
       "      <td>great</td>\n",
       "      <td>forward</td>\n",
       "      <td>trading</td>\n",
       "      <td>techs</td>\n",
       "      <td>colombia</td>\n",
       "      <td>planned</td>\n",
       "      <td>swedish</td>\n",
       "      <td>qaeda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>buzz</td>\n",
       "      <td>moody</td>\n",
       "      <td>rating</td>\n",
       "      <td>announces</td>\n",
       "      <td>russia</td>\n",
       "      <td>qatar</td>\n",
       "      <td>brazilian</td>\n",
       "      <td>zone</td>\n",
       "      <td>ax</td>\n",
       "      <td>mark</td>\n",
       "      <td>insider</td>\n",
       "      <td>maintenance</td>\n",
       "      <td>svcs</td>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>men</td>\n",
       "      <td>ca</td>\n",
       "      <td>moody</td>\n",
       "      <td>downgrade</td>\n",
       "      <td>italy</td>\n",
       "      <td>research</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>fourth</td>\n",
       "      <td>transmission</td>\n",
       "      <td>trading</td>\n",
       "      <td>svcs</td>\n",
       "      <td>ignore</td>\n",
       "      <td>buzz</td>\n",
       "      <td>planned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>singles</td>\n",
       "      <td>los</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>subdebt</td>\n",
       "      <td>standings</td>\n",
       "      <td>equities</td>\n",
       "      <td>iron</td>\n",
       "      <td>francisco</td>\n",
       "      <td>outage</td>\n",
       "      <td>buzz</td>\n",
       "      <td>falcao</td>\n",
       "      <td>dealing</td>\n",
       "      <td>aud</td>\n",
       "      <td>maintenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>los</td>\n",
       "      <td>angeles</td>\n",
       "      <td>rdf</td>\n",
       "      <td>reviews</td>\n",
       "      <td>svcs</td>\n",
       "      <td>sweden</td>\n",
       "      <td>escape</td>\n",
       "      <td>frankfurt</td>\n",
       "      <td>pjm</td>\n",
       "      <td>ax</td>\n",
       "      <td>result</td>\n",
       "      <td>resolved</td>\n",
       "      <td>jpy</td>\n",
       "      <td>liga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wins</td>\n",
       "      <td>county</td>\n",
       "      <td>argentine</td>\n",
       "      <td>testing</td>\n",
       "      <td>baseball</td>\n",
       "      <td>raises</td>\n",
       "      <td>transmission</td>\n",
       "      <td>free</td>\n",
       "      <td>halt</td>\n",
       "      <td>bo</td>\n",
       "      <td>scorers</td>\n",
       "      <td>scorers</td>\n",
       "      <td>momentum</td>\n",
       "      <td>motorcycling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>summit</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>stable</td>\n",
       "      <td>la</td>\n",
       "      <td>grand</td>\n",
       "      <td>rdf</td>\n",
       "      <td>outage</td>\n",
       "      <td>freeport</td>\n",
       "      <td>lows</td>\n",
       "      <td>inr</td>\n",
       "      <td>la</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>ignore</td>\n",
       "      <td>scorers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rpt</td>\n",
       "      <td>assigns</td>\n",
       "      <td>assigns</td>\n",
       "      <td>baseball</td>\n",
       "      <td>prix</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>pjm</td>\n",
       "      <td>founder</td>\n",
       "      <td>buzz</td>\n",
       "      <td>halt</td>\n",
       "      <td>liga</td>\n",
       "      <td>svcs</td>\n",
       "      <td>baseball</td>\n",
       "      <td>baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>manufacturing</td>\n",
       "      <td>meal</td>\n",
       "      <td>outlook</td>\n",
       "      <td>liga</td>\n",
       "      <td>dealing</td>\n",
       "      <td>oman</td>\n",
       "      <td>cancer</td>\n",
       "      <td>outlook</td>\n",
       "      <td>women</td>\n",
       "      <td>transmission</td>\n",
       "      <td>transmission</td>\n",
       "      <td>la</td>\n",
       "      <td>argentine</td>\n",
       "      <td>transmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>brief</td>\n",
       "      <td>baseball</td>\n",
       "      <td>championship</td>\n",
       "      <td>rdf</td>\n",
       "      <td>rdf</td>\n",
       "      <td>svcs</td>\n",
       "      <td>standings</td>\n",
       "      <td>weather</td>\n",
       "      <td>ks</td>\n",
       "      <td>outage</td>\n",
       "      <td>outage</td>\n",
       "      <td>liga</td>\n",
       "      <td>championship</td>\n",
       "      <td>outage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>vs</td>\n",
       "      <td>rapeseed</td>\n",
       "      <td>standings</td>\n",
       "      <td>scorers</td>\n",
       "      <td>motorcycling</td>\n",
       "      <td>resolved</td>\n",
       "      <td>championship</td>\n",
       "      <td>baseball</td>\n",
       "      <td>singles</td>\n",
       "      <td>pjm</td>\n",
       "      <td>pjm</td>\n",
       "      <td>rdf</td>\n",
       "      <td>standings</td>\n",
       "      <td>pjm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1             2             3             4   \\\n",
       "0       economics         mail          prix           ric        forced   \n",
       "1           sunni          rdf       dealing          svcs         force   \n",
       "2        includes      dealing            rs         moody          foot   \n",
       "3            pmis         svcs      resolved  motorcycling         hands   \n",
       "4             buy       rating        russia       ratings       founder   \n",
       "5            buzz        moody        rating     announces        russia   \n",
       "6             men           ca         moody     downgrade         italy   \n",
       "7         singles          los  motorcycling       subdebt     standings   \n",
       "8             los      angeles           rdf       reviews          svcs   \n",
       "9            wins       county     argentine       testing      baseball   \n",
       "10         summit  unconfirmed        stable            la         grand   \n",
       "11            rpt      assigns       assigns      baseball          prix   \n",
       "12  manufacturing         meal       outlook          liga       dealing   \n",
       "13          brief     baseball  championship           rdf           rdf   \n",
       "14             vs     rapeseed     standings       scorers  motorcycling   \n",
       "\n",
       "              5             6          7             8             9   \\\n",
       "0        weather     unchanged      forex     downgrade        digest   \n",
       "1       baseball           nzd       form       subdebt       dealing   \n",
       "2      standings          buzz      forum       reviews         block   \n",
       "3   championship           man   friendly           usd           ric   \n",
       "4          diary         great    forward       trading         techs   \n",
       "5          qatar     brazilian       zone            ax          mark   \n",
       "6       research   unconfirmed     fourth  transmission       trading   \n",
       "7       equities          iron  francisco        outage          buzz   \n",
       "8         sweden        escape  frankfurt           pjm            ax   \n",
       "9         raises  transmission       free          halt            bo   \n",
       "10           rdf        outage   freeport          lows           inr   \n",
       "11   unconfirmed           pjm    founder          buzz          halt   \n",
       "12          oman        cancer    outlook         women  transmission   \n",
       "13          svcs     standings    weather            ks        outage   \n",
       "14      resolved  championship   baseball       singles           pjm   \n",
       "\n",
       "              10           11            12            13  \n",
       "0            rdf    forecasts           rdf          prix  \n",
       "1         ignore        forex   unconfirmed       created  \n",
       "2            nhl         form       charges           rdf  \n",
       "3        message        forum       message        ignore  \n",
       "4       colombia      planned       swedish         qaeda  \n",
       "5        insider  maintenance          svcs            la  \n",
       "6           svcs       ignore          buzz       planned  \n",
       "7         falcao      dealing           aud   maintenance  \n",
       "8         result     resolved           jpy          liga  \n",
       "9        scorers      scorers      momentum  motorcycling  \n",
       "10            la  unconfirmed        ignore       scorers  \n",
       "11          liga         svcs      baseball      baseball  \n",
       "12  transmission           la     argentine  transmission  \n",
       "13        outage         liga  championship        outage  \n",
       "14           pjm          rdf     standings           pjm  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save to dataframe\n",
    "result = pd.DataFrame.from_dict({k : v['vocab'].tolist() for k, v in enumerate(result) })\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
